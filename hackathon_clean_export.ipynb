{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b5d679",
   "metadata": {},
   "source": [
    "# Hackathon Scouting América\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4e943",
   "metadata": {},
   "source": [
    "\n",
    "### 0. Imports & get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367b495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "from statsbombpy import sb\n",
    "from mplsoccer.pitch import VerticalPitch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "# ensure it loads from repo root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca07f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=Path(\".\").resolve().parent / \".env\")\n",
    "\n",
    "email = os.getenv(\"SB_USERNAME\")\n",
    "password = os.getenv(\"SB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbacf102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competition_id</th>\n",
       "      <th>season_id</th>\n",
       "      <th>country_name</th>\n",
       "      <th>competition_name</th>\n",
       "      <th>competition_gender</th>\n",
       "      <th>competition_youth</th>\n",
       "      <th>competition_international</th>\n",
       "      <th>season_name</th>\n",
       "      <th>match_updated</th>\n",
       "      <th>match_updated_360</th>\n",
       "      <th>match_available_360</th>\n",
       "      <th>match_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>317</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Liga MX</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/2025</td>\n",
       "      <td>2025-08-30T16:14:20.970616</td>\n",
       "      <td>2025-08-30T16:14:20.970616</td>\n",
       "      <td>2025-08-30T16:14:20.970616</td>\n",
       "      <td>2025-08-30T16:14:20.970616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>281</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Liga MX</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2023/2024</td>\n",
       "      <td>2024-12-20T23:40:31.103974</td>\n",
       "      <td>2024-12-20T23:40:31.103974</td>\n",
       "      <td>2024-12-20T23:40:31.103974</td>\n",
       "      <td>2024-12-20T23:40:31.103974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>235</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Liga MX</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2022/2023</td>\n",
       "      <td>2024-09-28T11:05:11.667984</td>\n",
       "      <td>2024-09-28T11:05:11.667984</td>\n",
       "      <td>2024-09-28T11:05:11.667984</td>\n",
       "      <td>2024-09-28T11:05:11.667984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>108</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Liga MX</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021/2022</td>\n",
       "      <td>2024-12-10T08:59:57.612449</td>\n",
       "      <td>2024-12-10T08:59:57.612449</td>\n",
       "      <td>2024-12-10T08:59:57.612449</td>\n",
       "      <td>2024-12-10T08:59:57.612449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   competition_id  season_id country_name competition_name competition_gender  \\\n",
       "0              73        317       Mexico          Liga MX               male   \n",
       "1              73        281       Mexico          Liga MX               male   \n",
       "2              73        235       Mexico          Liga MX               male   \n",
       "3              73        108       Mexico          Liga MX               male   \n",
       "\n",
       "   competition_youth  competition_international season_name  \\\n",
       "0              False                      False   2024/2025   \n",
       "1              False                      False   2023/2024   \n",
       "2              False                      False   2022/2023   \n",
       "3              False                      False   2021/2022   \n",
       "\n",
       "                match_updated           match_updated_360  \\\n",
       "0  2025-08-30T16:14:20.970616  2025-08-30T16:14:20.970616   \n",
       "1  2024-12-20T23:40:31.103974  2024-12-20T23:40:31.103974   \n",
       "2  2024-09-28T11:05:11.667984  2024-09-28T11:05:11.667984   \n",
       "3  2024-12-10T08:59:57.612449  2024-12-10T08:59:57.612449   \n",
       "\n",
       "          match_available_360             match_available  \n",
       "0  2025-08-30T16:14:20.970616  2025-08-30T16:14:20.970616  \n",
       "1  2024-12-20T23:40:31.103974  2024-12-20T23:40:31.103974  \n",
       "2  2024-09-28T11:05:11.667984  2024-09-28T11:05:11.667984  \n",
       "3  2024-12-10T08:59:57.612449  2024-12-10T08:59:57.612449  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.competitions(creds={\"user\": email, \"passwd\": password})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733e3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch matches\n",
    "get_data = False\n",
    "if get_data: \n",
    "    matches_2122 = sb.matches(competition_id=73, season_id=108, creds={\"user\": email, \"passwd\": password})\n",
    "    matches_2223 = sb.matches(competition_id=73, season_id=235, creds={\"user\": email, \"passwd\": password})\n",
    "    matches_2324 = sb.matches(competition_id=73, season_id=281, creds={\"user\": email, \"passwd\": password})\n",
    "    matches_2425 = sb.matches(competition_id=73, season_id=317, creds={\"user\": email, \"passwd\": password})\n",
    "    matches_2122.to_csv('data/matches_2122.csv')\n",
    "    matches_2223.to_csv('data/matches_2223.csv')\n",
    "    matches_2324.to_csv('data/matches_2324.csv')\n",
    "    matches_2425.to_csv('data/matches_2425.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58468691",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2122 = pd.read_csv('data/matches_2122.csv')\n",
    "matches_2223 = pd.read_csv('data/matches_2223.csv')\n",
    "matches_2324 = pd.read_csv('data/matches_2324.csv')\n",
    "matches_2425 = pd.read_csv('data/matches_2425.csv')\n",
    "matches_all = pd.concat([matches_2122, matches_2223, matches_2324, matches_2425], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d60ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsbombpy import sb\n",
    "import pandas as pd\n",
    "\n",
    "def build_unique_players_simple(matches_all: pd.DataFrame, email: str, password: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple dedupe by player_id across all matches:\n",
    "    - Iterates over home_team and away_team for each match_id\n",
    "    - Grabs the lineup DF for that team\n",
    "    - Stores the first non-null value seen for each player field\n",
    "    - Keeps a set of teams and jersey numbers (collapsed at the end)\n",
    "    \"\"\"\n",
    "    assert {\"match_id\",\"home_team\",\"away_team\"}.issubset(matches_all.columns)\n",
    "\n",
    "    creds = {\"user\": email, \"passwd\": password}\n",
    "    players = {}  # player_id -> dict of fields\n",
    "\n",
    "    def first_nonnull(target_dict, key, new_val):\n",
    "        if key not in target_dict or pd.isna(target_dict[key]):\n",
    "            target_dict[key] = new_val\n",
    "\n",
    "    for _, row in matches_all[[\"match_id\",\"home_team\",\"away_team\"]].dropna().iterrows():\n",
    "        mid = int(row[\"match_id\"])\n",
    "        home = str(row[\"home_team\"])\n",
    "        away = str(row[\"away_team\"])\n",
    "        print(\"Getting match \"+str(mid))\n",
    "\n",
    "        try:\n",
    "            lineups = sb.lineups(match_id=mid, creds=creds)  # {team_name: df}\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] lineups failed for match_id={mid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for team in (home, away):\n",
    "            df = lineups.get(team)\n",
    "            if df is None:\n",
    "                # Fallback: if the key doesn't match exactly, process all teams\n",
    "                for _, df_any in lineups.items():\n",
    "                    for _, r in df_any.iterrows():\n",
    "                        pid = int(r.get(\"player_id\"))\n",
    "                        pl = players.setdefault(pid, {\n",
    "                            \"player_id\": pid,\n",
    "                            \"player_name\": pd.NA,\n",
    "                            \"player_nickname\": pd.NA,\n",
    "                            \"birth_date\": pd.NA,\n",
    "                            \"player_gender\": pd.NA,\n",
    "                            \"player_height\": pd.NA,\n",
    "                            \"player_weight\": pd.NA,\n",
    "                            \"country\": pd.NA,\n",
    "                            \"teams_seen\": set(),\n",
    "                            \"jersey_numbers\": set()\n",
    "                        })\n",
    "                        first_nonnull(pl, \"player_name\", r.get(\"player_name\"))\n",
    "                        first_nonnull(pl, \"player_nickname\", r.get(\"player_nickname\"))\n",
    "                        first_nonnull(pl, \"birth_date\", r.get(\"birth_date\"))\n",
    "                        first_nonnull(pl, \"player_gender\", r.get(\"player_gender\"))\n",
    "                        first_nonnull(pl, \"player_height\", r.get(\"player_height\"))\n",
    "                        first_nonnull(pl, \"player_weight\", r.get(\"player_weight\"))\n",
    "\n",
    "                        country_val = r.get(\"country\")\n",
    "                        if isinstance(country_val, dict):\n",
    "                            country_val = country_val.get(\"name\") or country_val.get(\"country_name\") or str(country_val)\n",
    "                        first_nonnull(pl, \"country\", country_val)\n",
    "\n",
    "                        # jersey numbers may be mixed\n",
    "                        jn = pd.to_numeric(r.get(\"jersey_number\"), errors=\"coerce\")\n",
    "                        if pd.notna(jn):\n",
    "                            pl[\"jersey_numbers\"].add(int(jn))\n",
    "\n",
    "                        # No reliable team name here since we’re in fallback; skip teams_seen\n",
    "                continue\n",
    "\n",
    "            # Normal path: exact team match\n",
    "            for _, r in df.iterrows():\n",
    "                pid = int(r.get(\"player_id\"))\n",
    "                pl = players.setdefault(pid, {\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": pd.NA,\n",
    "                    \"player_nickname\": pd.NA,\n",
    "                    \"birth_date\": pd.NA,\n",
    "                    \"player_gender\": pd.NA,\n",
    "                    \"player_height\": pd.NA,\n",
    "                    \"player_weight\": pd.NA,\n",
    "                    \"country\": pd.NA,\n",
    "                    \"teams_seen\": set(),\n",
    "                    \"jersey_numbers\": set()\n",
    "                })\n",
    "                first_nonnull(pl, \"player_name\", r.get(\"player_name\"))\n",
    "                first_nonnull(pl, \"player_nickname\", r.get(\"player_nickname\"))\n",
    "                first_nonnull(pl, \"birth_date\", r.get(\"birth_date\"))\n",
    "                first_nonnull(pl, \"player_gender\", r.get(\"player_gender\"))\n",
    "                first_nonnull(pl, \"player_height\", r.get(\"player_height\"))\n",
    "                first_nonnull(pl, \"player_weight\", r.get(\"player_weight\"))\n",
    "\n",
    "                country_val = r.get(\"country\")\n",
    "                if isinstance(country_val, dict):\n",
    "                    country_val = country_val.get(\"name\") or country_val.get(\"country_name\") or str(country_val)\n",
    "                first_nonnull(pl, \"country\", country_val)\n",
    "\n",
    "                jn = pd.to_numeric(r.get(\"jersey_number\"), errors=\"coerce\")\n",
    "                if pd.notna(jn):\n",
    "                    pl[\"jersey_numbers\"].add(int(jn))\n",
    "\n",
    "                pl[\"teams_seen\"].add(team)\n",
    "\n",
    "    if not players:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"player_id\",\"player_name\",\"player_nickname\",\"birth_date\",\"player_gender\",\n",
    "            \"player_height\",\"player_weight\",\"country\",\"teams_seen\",\"jersey_numbers\"\n",
    "        ])\n",
    "\n",
    "    # Collapse sets and build the frame\n",
    "    records = []\n",
    "    for pl in players.values():\n",
    "        records.append({\n",
    "            \"player_id\": pl[\"player_id\"],\n",
    "            \"player_name\": pl[\"player_name\"],\n",
    "            \"player_nickname\": pl[\"player_nickname\"],\n",
    "            \"birth_date\": pl[\"birth_date\"],\n",
    "            \"player_gender\": pl[\"player_gender\"],\n",
    "            \"player_height\": pl[\"player_height\"],\n",
    "            \"player_weight\": pl[\"player_weight\"],\n",
    "            \"country\": pl[\"country\"],\n",
    "            \"teams_seen\": \"|\".join(sorted(pl[\"teams_seen\"])) if isinstance(pl[\"teams_seen\"], set) else pl[\"teams_seen\"],\n",
    "            \"jersey_numbers\": \"|\".join(map(str, sorted(pl[\"jersey_numbers\"]))) if isinstance(pl[\"jersey_numbers\"], set) else pl[\"jersey_numbers\"],\n",
    "        })\n",
    "\n",
    "    df_out = pd.DataFrame.from_records(records).sort_values([\"player_name\",\"player_id\"], na_position=\"last\").reset_index(drop=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0b9e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting match 3799447\n",
      "Getting match 3799496\n",
      "Getting match 3799494\n",
      "Getting match 3816669\n",
      "Getting match 3799475\n",
      "Getting match 3799356\n",
      "Getting match 3816756\n",
      "Getting match 3816081\n",
      "Getting match 3816674\n",
      "Getting match 3799470\n",
      "Getting match 3816722\n",
      "Getting match 3816685\n",
      "Getting match 3816696\n",
      "Getting match 3799450\n",
      "Getting match 3816796\n",
      "Getting match 3816721\n",
      "Getting match 3815592\n",
      "Getting match 3816673\n",
      "Getting match 3814881\n",
      "Getting match 3799365\n",
      "Getting match 3799481\n",
      "Getting match 3799497\n",
      "Getting match 3799401\n",
      "Getting match 3816812\n",
      "Getting match 3816695\n",
      "Getting match 3799499\n",
      "Getting match 3816807\n",
      "Getting match 3831472\n",
      "Getting match 3799456\n",
      "Getting match 3816694\n",
      "Getting match 3799432\n",
      "Getting match 3816816\n",
      "Getting match 3816693\n",
      "Getting match 3799360\n",
      "Getting match 3831926\n",
      "Getting match 3816763\n",
      "Getting match 3799388\n",
      "Getting match 3799467\n",
      "Getting match 3799383\n",
      "Getting match 3816730\n",
      "Getting match 3799441\n",
      "Getting match 3799434\n",
      "Getting match 3799490\n",
      "Getting match 3799381\n",
      "Getting match 3816778\n",
      "Getting match 3816785\n",
      "Getting match 3833794\n",
      "Getting match 3799400\n",
      "Getting match 3799364\n",
      "Getting match 3816666\n",
      "Getting match 3799463\n",
      "Getting match 3816757\n",
      "Getting match 3799488\n",
      "Getting match 3816697\n",
      "Getting match 3816712\n",
      "Getting match 3815230\n",
      "Getting match 3816815\n",
      "Getting match 3816739\n",
      "Getting match 3799443\n",
      "Getting match 3816773\n",
      "Getting match 3816665\n",
      "Getting match 3816691\n",
      "Getting match 3816793\n",
      "Getting match 3816689\n",
      "Getting match 3816781\n",
      "Getting match 3799384\n",
      "Getting match 3799367\n",
      "Getting match 3799370\n",
      "Getting match 3799446\n",
      "Getting match 3799502\n",
      "Getting match 3799471\n",
      "Getting match 3799426\n",
      "Getting match 3799493\n",
      "Getting match 3831922\n",
      "Getting match 3816681\n",
      "Getting match 3832382\n",
      "Getting match 3799440\n",
      "Getting match 3799444\n",
      "Getting match 3816716\n",
      "Getting match 3816718\n",
      "Getting match 3799396\n",
      "Getting match 3831919\n",
      "Getting match 3816671\n",
      "Getting match 3816800\n",
      "Getting match 3816706\n",
      "Getting match 3816765\n",
      "Getting match 3816774\n",
      "Getting match 3815594\n",
      "Getting match 3816802\n",
      "Getting match 3799406\n",
      "Getting match 3799368\n",
      "Getting match 3816787\n",
      "Getting match 3816748\n",
      "Getting match 3816678\n",
      "Getting match 3799482\n",
      "Getting match 3816813\n",
      "Getting match 3816714\n",
      "Getting match 3799421\n",
      "Getting match 3799387\n",
      "Getting match 3816779\n",
      "Getting match 3799486\n",
      "Getting match 3799371\n",
      "Getting match 3799455\n",
      "Getting match 3799484\n",
      "Getting match 3816745\n",
      "Getting match 3816727\n",
      "Getting match 3799478\n",
      "Getting match 3799369\n",
      "Getting match 3816729\n",
      "Getting match 3799464\n",
      "Getting match 3799389\n",
      "Getting match 3816683\n",
      "Getting match 3831469\n",
      "Getting match 3799420\n",
      "Getting match 3799392\n",
      "Getting match 3799363\n",
      "Getting match 3799473\n",
      "Getting match 3799414\n",
      "Getting match 3816754\n",
      "Getting match 3816720\n",
      "Getting match 3799405\n",
      "Getting match 3816713\n",
      "Getting match 3816686\n",
      "Getting match 3799399\n",
      "Getting match 3816740\n",
      "Getting match 3816788\n",
      "Getting match 3816786\n",
      "Getting match 3816664\n",
      "Getting match 3799380\n",
      "Getting match 3816798\n",
      "Getting match 3799361\n",
      "Getting match 3831920\n",
      "Getting match 3816758\n",
      "Getting match 3799409\n",
      "Getting match 3816806\n",
      "Getting match 3799492\n",
      "Getting match 3799451\n",
      "Getting match 3816725\n",
      "Getting match 3799433\n",
      "Getting match 3799394\n",
      "Getting match 3799358\n",
      "Getting match 3799485\n",
      "Getting match 3816809\n",
      "Getting match 3816670\n",
      "Getting match 3799355\n",
      "Getting match 3816762\n",
      "Getting match 3799487\n",
      "Getting match 3799391\n",
      "Getting match 3799374\n",
      "Getting match 3799479\n",
      "Getting match 3816679\n",
      "Getting match 3799378\n",
      "Getting match 3816728\n",
      "Getting match 3816808\n",
      "Getting match 3799501\n",
      "Getting match 3816677\n",
      "Getting match 3799480\n",
      "Getting match 3816760\n",
      "Getting match 3799462\n",
      "Getting match 3816741\n",
      "Getting match 3799445\n",
      "Getting match 3816766\n",
      "Getting match 3816672\n",
      "Getting match 3816710\n",
      "Getting match 3816724\n",
      "Getting match 3816690\n",
      "Getting match 3816692\n",
      "Getting match 3816700\n",
      "Getting match 3816675\n",
      "Getting match 3816668\n",
      "Getting match 3799407\n",
      "Getting match 3816772\n",
      "Getting match 3799465\n",
      "Getting match 3799386\n",
      "Getting match 3799402\n",
      "Getting match 3799366\n",
      "Getting match 3816784\n",
      "Getting match 3799430\n",
      "Getting match 3816734\n",
      "Getting match 3799460\n",
      "Getting match 3816790\n",
      "Getting match 3816698\n",
      "Getting match 3816736\n",
      "Getting match 3816708\n",
      "Getting match 3816688\n",
      "Getting match 3816750\n",
      "Getting match 3816761\n",
      "Getting match 3816682\n",
      "Getting match 3816804\n",
      "Getting match 3816795\n",
      "Getting match 3816723\n",
      "Getting match 3816770\n",
      "Getting match 3815593\n",
      "Getting match 3816732\n",
      "Getting match 3799454\n",
      "Getting match 3816755\n",
      "Getting match 3799442\n",
      "Getting match 3816082\n",
      "Getting match 3799390\n",
      "Getting match 3799362\n",
      "Getting match 3816731\n",
      "Getting match 3799424\n",
      "Getting match 3816709\n",
      "Getting match 3799385\n",
      "Getting match 3816777\n",
      "Getting match 3831923\n",
      "Getting match 3799373\n",
      "Getting match 3799357\n",
      "Getting match 3815591\n",
      "Getting match 3816810\n",
      "Getting match 3799491\n",
      "Getting match 3816699\n",
      "Getting match 3799415\n",
      "Getting match 3799422\n",
      "Getting match 3799395\n",
      "Getting match 3799498\n",
      "Getting match 3799398\n",
      "Getting match 3799359\n",
      "Getting match 3799500\n",
      "Getting match 3799495\n",
      "Getting match 3799448\n",
      "Getting match 3816707\n",
      "Getting match 3799436\n",
      "Getting match 3816719\n",
      "Getting match 3816811\n",
      "Getting match 3816733\n",
      "Getting match 3799435\n",
      "Getting match 3800621\n",
      "Getting match 3799372\n",
      "Getting match 3799412\n",
      "Getting match 3832381\n",
      "Getting match 3799474\n",
      "Getting match 3831471\n",
      "Getting match 3831924\n",
      "Getting match 3831470\n",
      "Getting match 3816799\n",
      "Getting match 3816737\n",
      "Getting match 3816744\n",
      "Getting match 3816769\n",
      "Getting match 3799397\n",
      "Getting match 3816717\n",
      "Getting match 3799429\n",
      "Getting match 3815234\n",
      "Getting match 3799375\n",
      "Getting match 3799417\n",
      "Getting match 3799466\n",
      "Getting match 3799457\n",
      "Getting match 3799408\n",
      "Getting match 3799431\n",
      "Getting match 3799469\n",
      "Getting match 3799411\n",
      "Getting match 3831921\n",
      "Getting match 3799413\n",
      "Getting match 3833795\n",
      "Getting match 3799418\n",
      "Getting match 3832383\n",
      "Getting match 3816775\n",
      "Getting match 3816753\n",
      "Getting match 3816803\n",
      "Getting match 3799438\n",
      "Getting match 3816764\n",
      "Getting match 3815229\n",
      "Getting match 3799382\n",
      "Getting match 3816738\n",
      "Getting match 3799352\n",
      "Getting match 3816703\n",
      "Getting match 3799419\n",
      "Getting match 3816743\n",
      "Getting match 3816680\n",
      "Getting match 3816814\n",
      "Getting match 3816771\n",
      "Getting match 3816742\n",
      "Getting match 3799472\n",
      "Getting match 3816735\n",
      "Getting match 3816704\n",
      "Getting match 3816797\n",
      "Getting match 3799477\n",
      "Getting match 3816702\n",
      "Getting match 3799461\n",
      "Getting match 3816792\n",
      "Getting match 3816801\n",
      "Getting match 3816751\n",
      "Getting match 3816789\n",
      "Getting match 3816746\n",
      "Getting match 3816776\n",
      "Getting match 3815233\n",
      "Getting match 3816701\n",
      "Getting match 3799393\n",
      "Getting match 3816794\n",
      "Getting match 3799351\n",
      "Getting match 3816780\n",
      "Getting match 3799416\n",
      "Getting match 3816767\n",
      "Getting match 3799376\n",
      "Getting match 3799354\n",
      "Getting match 3799453\n",
      "Getting match 3799439\n",
      "Getting match 3816791\n",
      "Getting match 3799483\n",
      "Getting match 3815235\n",
      "Getting match 3799377\n",
      "Getting match 3814880\n",
      "Getting match 3799379\n",
      "Getting match 3799449\n",
      "Getting match 3799489\n",
      "Getting match 3831925\n",
      "Getting match 3799476\n",
      "Getting match 3816667\n",
      "Getting match 3832384\n",
      "Getting match 3816705\n",
      "Getting match 3816752\n",
      "Getting match 3816768\n",
      "Getting match 3816747\n",
      "Getting match 3816783\n",
      "Getting match 3816805\n",
      "Getting match 3816759\n",
      "Getting match 3815231\n",
      "Getting match 3815232\n",
      "Getting match 3799458\n",
      "Getting match 3815228\n",
      "Getting match 3799428\n",
      "Getting match 3814882\n",
      "Getting match 3816711\n",
      "Getting match 3799410\n",
      "Getting match 3816684\n",
      "Getting match 3799437\n",
      "Getting match 3814879\n",
      "Getting match 3816782\n",
      "Getting match 3799459\n",
      "Getting match 3816687\n",
      "Getting match 3799423\n",
      "Getting match 3816726\n",
      "Getting match 3799452\n",
      "Getting match 3799468\n",
      "Getting match 3799403\n",
      "Getting match 3799404\n",
      "Getting match 3799353\n",
      "Getting match 3799425\n",
      "Getting match 3816749\n",
      "Getting match 3816676\n",
      "Getting match 3816715\n",
      "Getting match 3799427\n",
      "Getting match 3835193\n",
      "Getting match 3886545\n",
      "Getting match 3870488\n",
      "Getting match 3870518\n",
      "Getting match 3870478\n",
      "Getting match 3870436\n",
      "Getting match 3835117\n",
      "Getting match 3870429\n",
      "Getting match 3835238\n",
      "Getting match 3870421\n",
      "Getting match 3835224\n",
      "Getting match 3870379\n",
      "Getting match 3870473\n",
      "Getting match 3835251\n",
      "Getting match 3870468\n",
      "Getting match 3835173\n",
      "Getting match 3886182\n",
      "Getting match 3870456\n",
      "Getting match 3835147\n",
      "Getting match 3870406\n",
      "Getting match 3835127\n",
      "Getting match 3835214\n",
      "Getting match 3835250\n",
      "Getting match 3835143\n",
      "Getting match 3870508\n",
      "Getting match 3835126\n",
      "Getting match 3835134\n",
      "Getting match 3835179\n",
      "Getting match 3835225\n",
      "Getting match 3870410\n",
      "Getting match 3835106\n",
      "Getting match 3870462\n",
      "Getting match 3870442\n",
      "Getting match 3835180\n",
      "Getting match 3870400\n",
      "Getting match 3870474\n",
      "Getting match 3835212\n",
      "Getting match 3870443\n",
      "Getting match 3835181\n",
      "Getting match 3870390\n",
      "Getting match 3870448\n",
      "Getting match 3835211\n",
      "Getting match 3870469\n",
      "Getting match 3870384\n",
      "Getting match 3870418\n",
      "Getting match 3835190\n",
      "Getting match 3870465\n",
      "Getting match 3835232\n",
      "Getting match 3870417\n",
      "Getting match 3835138\n",
      "Getting match 3835175\n",
      "Getting match 3835203\n",
      "Getting match 3864093\n",
      "Getting match 3835113\n",
      "Getting match 3870437\n",
      "Getting match 3835233\n",
      "Getting match 3835246\n",
      "Getting match 3870493\n",
      "Getting match 3870430\n",
      "Getting match 3835165\n",
      "Getting match 3870514\n",
      "Getting match 3870510\n",
      "Getting match 3870381\n",
      "Getting match 3870515\n",
      "Getting match 3870509\n",
      "Getting match 3870502\n",
      "Getting match 3864709\n",
      "Getting match 3870383\n",
      "Getting match 3870492\n",
      "Getting match 3870471\n",
      "Getting match 3886180\n",
      "Getting match 3870457\n",
      "Getting match 3870376\n",
      "Getting match 3835112\n",
      "Getting match 3870423\n",
      "https://data.statsbombservices.com/api/v4/lineups/3870423 -> 502\n",
      "Getting match 3886544\n",
      "Getting match 3870484\n",
      "Getting match 3835170\n",
      "Getting match 3835178\n",
      "Getting match 3835150\n",
      "Getting match 3835167\n",
      "Getting match 3835158\n",
      "Getting match 3887190\n",
      "Getting match 3886184\n",
      "Getting match 3835154\n",
      "Getting match 3870524\n",
      "Getting match 3835144\n",
      "Getting match 3835122\n",
      "Getting match 3870513\n",
      "Getting match 3835131\n",
      "Getting match 3835226\n",
      "Getting match 3870433\n",
      "Getting match 3870485\n",
      "Getting match 3835230\n",
      "Getting match 3870380\n",
      "Getting match 3870480\n",
      "Getting match 3835200\n",
      "Getting match 3835120\n",
      "Getting match 3870450\n",
      "Getting match 3835199\n",
      "Getting match 3835111\n",
      "Getting match 3870414\n",
      "Getting match 3835114\n",
      "Getting match 3835101\n",
      "Getting match 3870387\n",
      "Getting match 3835109\n",
      "Getting match 3864710\n",
      "Getting match 3870386\n",
      "Getting match 3885771\n",
      "Getting match 3864095\n",
      "Getting match 3864708\n",
      "Getting match 3885770\n",
      "Getting match 3835216\n",
      "Getting match 3835245\n",
      "Getting match 3870525\n",
      "Getting match 3835209\n",
      "Getting match 3835155\n",
      "Getting match 3870519\n",
      "Getting match 3835187\n",
      "Getting match 3870523\n",
      "Getting match 3870464\n",
      "Getting match 3835172\n",
      "Getting match 3835169\n",
      "Getting match 3870458\n",
      "Getting match 3835130\n",
      "Getting match 3835234\n",
      "Getting match 3870441\n",
      "Getting match 3835121\n",
      "Getting match 3835141\n",
      "Getting match 3870408\n",
      "Getting match 3870434\n",
      "Getting match 3886185\n",
      "Getting match 3870396\n",
      "Getting match 3835196\n",
      "Getting match 3870503\n",
      "Getting match 3835253\n",
      "Getting match 3835194\n",
      "Getting match 3870428\n",
      "Getting match 3835174\n",
      "Getting match 3870432\n",
      "Getting match 3835207\n",
      "Getting match 3835149\n",
      "Getting match 3835219\n",
      "Getting match 3835104\n",
      "Getting match 3835237\n",
      "Getting match 3835118\n",
      "Getting match 3864712\n",
      "Getting match 3870445\n",
      "Getting match 3835115\n",
      "Getting match 3835242\n",
      "Getting match 3870416\n",
      "Getting match 3870409\n",
      "Getting match 3835229\n",
      "Getting match 3870394\n",
      "Getting match 3870385\n",
      "Getting match 3870399\n",
      "Getting match 3870382\n",
      "Getting match 3886547\n",
      "Getting match 3870527\n",
      "Getting match 3886546\n",
      "Getting match 3870470\n",
      "Getting match 3870504\n",
      "Getting match 3886179\n",
      "Getting match 3835123\n",
      "Getting match 3870439\n",
      "Getting match 3835244\n",
      "Getting match 3870412\n",
      "Getting match 3835236\n",
      "Getting match 3870398\n",
      "Getting match 3870501\n",
      "Getting match 3835102\n",
      "Getting match 3870487\n",
      "Getting match 3870460\n",
      "Getting match 3835206\n",
      "Getting match 3870403\n",
      "Getting match 3866628\n",
      "Getting match 3870388\n",
      "Getting match 3864711\n",
      "Getting match 3864714\n",
      "Getting match 3835248\n",
      "Getting match 3835249\n",
      "Getting match 3835247\n",
      "Getting match 3835241\n",
      "Getting match 3835215\n",
      "Getting match 3835208\n",
      "Getting match 3835136\n",
      "Getting match 3835204\n",
      "Getting match 3835108\n",
      "Getting match 3835197\n",
      "Getting match 3870526\n",
      "Getting match 3835163\n",
      "Getting match 3870490\n",
      "Getting match 3835161\n",
      "Getting match 3870472\n",
      "Getting match 3835145\n",
      "Getting match 3870438\n",
      "Getting match 3835135\n",
      "Getting match 3870402\n",
      "Getting match 3870452\n",
      "Getting match 3870391\n",
      "Getting match 3870446\n",
      "Getting match 3870447\n",
      "Getting match 3870393\n",
      "Getting match 3870413\n",
      "Getting match 3835195\n",
      "Getting match 3870405\n",
      "Getting match 3835235\n",
      "Getting match 3870377\n",
      "Getting match 3886186\n",
      "Getting match 3835191\n",
      "Getting match 3835210\n",
      "Getting match 3870491\n",
      "Getting match 3835146\n",
      "Getting match 3835198\n",
      "Getting match 3870522\n",
      "Getting match 3835119\n",
      "Getting match 3870496\n",
      "Getting match 3886181\n",
      "Getting match 3870486\n",
      "Getting match 3870494\n",
      "Getting match 3870463\n",
      "Getting match 3886183\n",
      "Getting match 3835137\n",
      "Getting match 3870517\n",
      "Getting match 3835184\n",
      "Getting match 3870475\n",
      "Getting match 3835107\n",
      "Getting match 3870435\n",
      "Getting match 3835103\n",
      "Getting match 3870425\n",
      "Getting match 3870520\n",
      "Getting match 3866629\n",
      "Getting match 3870500\n",
      "Getting match 3864707\n",
      "Getting match 3870444\n",
      "Getting match 3864713\n",
      "Getting match 3865781\n",
      "Getting match 3835252\n",
      "Getting match 3835231\n",
      "Getting match 3835222\n",
      "Getting match 3870521\n",
      "Getting match 3835218\n",
      "Getting match 3870506\n",
      "Getting match 3835159\n",
      "Getting match 3870467\n",
      "Getting match 3870512\n",
      "Getting match 3835223\n",
      "Getting match 3870495\n",
      "Getting match 3835185\n",
      "Getting match 3870497\n",
      "Getting match 3835177\n",
      "Getting match 3870411\n",
      "Getting match 3835140\n",
      "Getting match 3870404\n",
      "Getting match 3835133\n",
      "Getting match 3870389\n",
      "Getting match 3870431\n",
      "Getting match 3835171\n",
      "Getting match 3870419\n",
      "Getting match 3835168\n",
      "Getting match 3870415\n",
      "Getting match 3835129\n",
      "Getting match 3835125\n",
      "Getting match 3870424\n",
      "Getting match 3870481\n",
      "Getting match 3864094\n",
      "Getting match 3835164\n",
      "Getting match 3835228\n",
      "Getting match 3870511\n",
      "Getting match 3835160\n",
      "Getting match 3870505\n",
      "Getting match 3870482\n",
      "Getting match 3835116\n",
      "Getting match 3835151\n",
      "Getting match 3870477\n",
      "Getting match 3835142\n",
      "Getting match 3870427\n",
      "Getting match 3870476\n",
      "Getting match 3835240\n",
      "Getting match 3870453\n",
      "Getting match 3870516\n",
      "Getting match 3887189\n",
      "Getting match 3870489\n",
      "Getting match 3885769\n",
      "Getting match 3870461\n",
      "Getting match 3870401\n",
      "Getting match 3870392\n",
      "Getting match 3870397\n",
      "Getting match 3864096\n",
      "Getting match 3835243\n",
      "Getting match 3835182\n",
      "Getting match 3835227\n",
      "Getting match 3835183\n",
      "Getting match 3835205\n",
      "Getting match 3835166\n",
      "Getting match 3835201\n",
      "Getting match 3835157\n",
      "Getting match 3835156\n",
      "Getting match 3835153\n",
      "Getting match 3835139\n",
      "Getting match 3835148\n",
      "Getting match 3835132\n",
      "Getting match 3885772\n",
      "Getting match 3870498\n",
      "Getting match 3870499\n",
      "Getting match 3870466\n",
      "Getting match 3870454\n",
      "Getting match 3870459\n",
      "Getting match 3870426\n",
      "Getting match 3870451\n",
      "Getting match 3870422\n",
      "Getting match 3870449\n",
      "Getting match 3865780\n",
      "Getting match 3870420\n",
      "Getting match 3835217\n",
      "Getting match 3870407\n",
      "Getting match 3835202\n",
      "Getting match 3870395\n",
      "Getting match 3835176\n",
      "Getting match 3835239\n",
      "Getting match 3835110\n",
      "Getting match 3835220\n",
      "Getting match 3835105\n",
      "Getting match 3835213\n",
      "Getting match 3835189\n",
      "Getting match 3835186\n",
      "Getting match 3835188\n",
      "Getting match 3835152\n",
      "Getting match 3835162\n",
      "Getting match 3835128\n",
      "Getting match 3835124\n",
      "Getting match 3870455\n",
      "Getting match 3870440\n",
      "Getting match 3870507\n",
      "Getting match 3870378\n",
      "Getting match 3870375\n",
      "Getting match 3865778\n",
      "Getting match 3865779\n",
      "Getting match 3835192\n",
      "Getting match 3835221\n",
      "Getting match 3870483\n",
      "Getting match 3870479\n",
      "Getting match 3889665\n",
      "Getting match 3919032\n",
      "Getting match 3889642\n",
      "Getting match 3918988\n",
      "Getting match 3889659\n",
      "Getting match 3918953\n",
      "Getting match 3919053\n",
      "Getting match 3917275\n",
      "Getting match 3919014\n",
      "Getting match 3889785\n",
      "Getting match 3919021\n",
      "Getting match 3889733\n",
      "Getting match 3918980\n",
      "Getting match 3889684\n",
      "Getting match 3889783\n",
      "Getting match 3889640\n",
      "Getting match 3889726\n",
      "Getting match 3919077\n",
      "Getting match 3889668\n",
      "Getting match 3919071\n",
      "Getting match 3889647\n",
      "Getting match 3918989\n",
      "Getting match 3919094\n",
      "Getting match 3889661\n",
      "Getting match 3919102\n",
      "Getting match 3919051\n",
      "Getting match 3918978\n",
      "Getting match 3919055\n",
      "Getting match 3936498\n",
      "Getting match 3917241\n",
      "Getting match 3889771\n",
      "Getting match 3889764\n",
      "Getting match 3917279\n",
      "Getting match 3919089\n",
      "Getting match 3919067\n",
      "Getting match 3918986\n",
      "Getting match 3889724\n",
      "Getting match 3889747\n",
      "Getting match 3919033\n",
      "Getting match 3919040\n",
      "Getting match 3918961\n",
      "Getting match 3889708\n",
      "Getting match 3889744\n",
      "Getting match 3918984\n",
      "Getting match 3919025\n",
      "Getting match 3889711\n",
      "Getting match 3889654\n",
      "Getting match 3889716\n",
      "Getting match 3918968\n",
      "Getting match 3889748\n",
      "Getting match 3918374\n",
      "Getting match 3919058\n",
      "Getting match 3919015\n",
      "Getting match 3918970\n",
      "Getting match 3919001\n",
      "Getting match 3917859\n",
      "Getting match 3919007\n",
      "Getting match 3889662\n",
      "Getting match 3918954\n",
      "Getting match 3918982\n",
      "Getting match 3919012\n",
      "Getting match 3889652\n",
      "Getting match 3889648\n",
      "Getting match 3917278\n",
      "Getting match 3918965\n",
      "Getting match 3889672\n",
      "Getting match 3936499\n",
      "Getting match 3889687\n",
      "Getting match 3918976\n",
      "Getting match 3918993\n",
      "Getting match 3919098\n",
      "Getting match 3889686\n",
      "Getting match 3919069\n",
      "Getting match 3889657\n",
      "Getting match 3919095\n",
      "Getting match 3889725\n",
      "Getting match 3918959\n",
      "Getting match 3919079\n",
      "Getting match 3919047\n",
      "Getting match 3889738\n",
      "Getting match 3889723\n",
      "Getting match 3918990\n",
      "Getting match 3919013\n",
      "Getting match 3919046\n",
      "Getting match 3889688\n",
      "Getting match 3919074\n",
      "Getting match 3889780\n",
      "Getting match 3919042\n",
      "Getting match 3889685\n",
      "Getting match 3889697\n",
      "Getting match 3889775\n",
      "Getting match 3889712\n",
      "Getting match 3919043\n",
      "Getting match 3889786\n",
      "Getting match 3889742\n",
      "Getting match 3918999\n",
      "Getting match 3889671\n",
      "Getting match 3889781\n",
      "Getting match 3889663\n",
      "Getting match 3918973\n",
      "Getting match 3889772\n",
      "Getting match 3919065\n",
      "Getting match 3889784\n",
      "Getting match 3936497\n",
      "Getting match 3918977\n",
      "Getting match 3889782\n",
      "Getting match 3919096\n",
      "Getting match 3918971\n",
      "Getting match 3889763\n",
      "Getting match 3919086\n",
      "Getting match 3889735\n",
      "Getting match 3919061\n",
      "Getting match 3917858\n",
      "Getting match 3889734\n",
      "Getting match 3889756\n",
      "Getting match 3889773\n",
      "Getting match 3889751\n",
      "Getting match 3889755\n",
      "Getting match 3919078\n",
      "Getting match 3889728\n",
      "Getting match 3919052\n",
      "Getting match 3889718\n",
      "Getting match 3889666\n",
      "Getting match 3919016\n",
      "Getting match 3919070\n",
      "Getting match 3919092\n",
      "Getting match 3889759\n",
      "Getting match 3919050\n",
      "Getting match 3919091\n",
      "Getting match 3919080\n",
      "Getting match 3919028\n",
      "Getting match 3918962\n",
      "Getting match 3889689\n",
      "Getting match 3919009\n",
      "Getting match 3889727\n",
      "Getting match 3889679\n",
      "Getting match 3919000\n",
      "Getting match 3889699\n",
      "Getting match 3889644\n",
      "Getting match 3918981\n",
      "Getting match 3889676\n",
      "Getting match 3918964\n",
      "Getting match 3918974\n",
      "Getting match 3919066\n",
      "Getting match 3889761\n",
      "Getting match 3918956\n",
      "Getting match 3889678\n",
      "Getting match 3889788\n",
      "Getting match 3936264\n",
      "Getting match 3919036\n",
      "Getting match 3889674\n",
      "Getting match 3919093\n",
      "Getting match 3919017\n",
      "Getting match 3889660\n",
      "Getting match 3919073\n",
      "Getting match 3889680\n",
      "Getting match 3889641\n",
      "Getting match 3919031\n",
      "Getting match 3918997\n",
      "Getting match 3936496\n",
      "Getting match 3889750\n",
      "Getting match 3889651\n",
      "Getting match 3919101\n",
      "Getting match 3889704\n",
      "Getting match 3918963\n",
      "Getting match 3919075\n",
      "Getting match 3918979\n",
      "Getting match 3918952\n",
      "Getting match 3919057\n",
      "Getting match 3889752\n",
      "Getting match 3918375\n",
      "Getting match 3919035\n",
      "Getting match 3889710\n",
      "Getting match 3889789\n",
      "Getting match 3919034\n",
      "Getting match 3889696\n",
      "Getting match 3918991\n",
      "Getting match 3889658\n",
      "Getting match 3919076\n",
      "Getting match 3889650\n",
      "Getting match 3919062\n",
      "Getting match 3919029\n",
      "Getting match 3889653\n",
      "Getting match 3919030\n",
      "Getting match 3889637\n",
      "Getting match 3936617\n",
      "Getting match 3918957\n",
      "Getting match 3919044\n",
      "Getting match 3889757\n",
      "Getting match 3889681\n",
      "Getting match 3889736\n",
      "Getting match 3889693\n",
      "Getting match 3889749\n",
      "Getting match 3919084\n",
      "Getting match 3889745\n",
      "Getting match 3918995\n",
      "Getting match 3889706\n",
      "Getting match 3917280\n",
      "Getting match 3889702\n",
      "Getting match 3889779\n",
      "Getting match 3889677\n",
      "Getting match 3889776\n",
      "Getting match 3889692\n",
      "Getting match 3919090\n",
      "Getting match 3889655\n",
      "Getting match 3889709\n",
      "Getting match 3919097\n",
      "Getting match 3919083\n",
      "Getting match 3919060\n",
      "Getting match 3919039\n",
      "Getting match 3919049\n",
      "Getting match 3919004\n",
      "Getting match 3918969\n",
      "Getting match 3919008\n",
      "Getting match 3918960\n",
      "Getting match 3919005\n",
      "Getting match 3889787\n",
      "Getting match 3918985\n",
      "Getting match 3889762\n",
      "Getting match 3919064\n",
      "Getting match 3889753\n",
      "Getting match 3918975\n",
      "Getting match 3889721\n",
      "Getting match 3936501\n",
      "Getting match 3889643\n",
      "Getting match 3936265\n",
      "Getting match 3937184\n",
      "Getting match 3889768\n",
      "Getting match 3936618\n",
      "Getting match 3889765\n",
      "Getting match 3919022\n",
      "Getting match 3889754\n",
      "Getting match 3919011\n",
      "Getting match 3889740\n",
      "Getting match 3916236\n",
      "Getting match 3889707\n",
      "Getting match 3919041\n",
      "Getting match 3919099\n",
      "Getting match 3889770\n",
      "Getting match 3889691\n",
      "Getting match 3889766\n",
      "Getting match 3889682\n",
      "Getting match 3919037\n",
      "Getting match 3889664\n",
      "Getting match 3889729\n",
      "Getting match 3889778\n",
      "Getting match 3889683\n",
      "Getting match 3889741\n",
      "Getting match 3918987\n",
      "Getting match 3889714\n",
      "Getting match 3919081\n",
      "Getting match 3889675\n",
      "Getting match 3919045\n",
      "Getting match 3937694\n",
      "Getting match 3889760\n",
      "Getting match 3937186\n",
      "Getting match 3889694\n",
      "Getting match 3936495\n",
      "Getting match 3917857\n",
      "Getting match 3919100\n",
      "Getting match 3919026\n",
      "Getting match 3916237\n",
      "Getting match 3917453\n",
      "Getting match 3889667\n",
      "Getting match 3889695\n",
      "Getting match 3919072\n",
      "Getting match 3889769\n",
      "Getting match 3919056\n",
      "Getting match 3937185\n",
      "Getting match 3918972\n",
      "Getting match 3889739\n",
      "Getting match 3917277\n",
      "Getting match 3889656\n",
      "Getting match 3889717\n",
      "Getting match 3889638\n",
      "Getting match 3889713\n",
      "Getting match 3889715\n",
      "Getting match 3889690\n",
      "Getting match 3918967\n",
      "Getting match 3889670\n",
      "Getting match 3937187\n",
      "Getting match 3889746\n",
      "Getting match 3919087\n",
      "Getting match 3919063\n",
      "Getting match 3919054\n",
      "Getting match 3919024\n",
      "Getting match 3919023\n",
      "Getting match 3889722\n",
      "Getting match 3918992\n",
      "Getting match 3889673\n",
      "Getting match 3917856\n",
      "Getting match 3918955\n",
      "Getting match 3889758\n",
      "Getting match 3889774\n",
      "Getting match 3889701\n",
      "Getting match 3919048\n",
      "Getting match 3889700\n",
      "Getting match 3919027\n",
      "Getting match 3889669\n",
      "Getting match 3919020\n",
      "Getting match 3919059\n",
      "Getting match 3919006\n",
      "Getting match 3918966\n",
      "Getting match 3918996\n",
      "Getting match 3919019\n",
      "Getting match 3918983\n",
      "Getting match 3918998\n",
      "Getting match 3919103\n",
      "Getting match 3889705\n",
      "Getting match 3918958\n",
      "Getting match 3918951\n",
      "Getting match 3889777\n",
      "Getting match 3889703\n",
      "Getting match 3889743\n",
      "Getting match 3919088\n",
      "Getting match 3889698\n",
      "Getting match 3917276\n",
      "Getting match 3889649\n",
      "Getting match 3889720\n",
      "Getting match 3889646\n",
      "Getting match 3889732\n",
      "Getting match 3889645\n",
      "Getting match 3889719\n",
      "Getting match 3889639\n",
      "Getting match 3919082\n",
      "Getting match 3919085\n",
      "Getting match 3919038\n",
      "Getting match 3917452\n",
      "Getting match 3919018\n",
      "Getting match 3889731\n",
      "Getting match 3919010\n",
      "Getting match 3889730\n",
      "Getting match 3919003\n",
      "Getting match 3937695\n",
      "Getting match 3889767\n",
      "Getting match 3936500\n",
      "Getting match 3889737\n",
      "Getting match 3919068\n",
      "Getting match 3919002\n",
      "Getting match 3918994\n",
      "Getting match 3939904\n",
      "Getting match 3939883\n",
      "Getting match 3939872\n",
      "Getting match 3992815\n",
      "Getting match 3993197\n",
      "Getting match 3939821\n",
      "Getting match 3972140\n",
      "Getting match 3972048\n",
      "Getting match 3972049\n",
      "Getting match 3939884\n",
      "Getting match 3939916\n",
      "Getting match 3972164\n",
      "Getting match 3939906\n",
      "Getting match 3972021\n",
      "Getting match 3972131\n",
      "Getting match 3972054\n",
      "Getting match 3939914\n",
      "Getting match 3939903\n",
      "Getting match 3994839\n",
      "Getting match 3972127\n",
      "Getting match 3939963\n",
      "Getting match 3939908\n",
      "Getting match 3972147\n",
      "Getting match 3972092\n",
      "Getting match 3972123\n",
      "Getting match 3972059\n",
      "Getting match 3972017\n",
      "Getting match 3939894\n",
      "Getting match 3972075\n",
      "Getting match 3972163\n",
      "Getting match 3969168\n",
      "Getting match 3939901\n",
      "Getting match 3972083\n",
      "Getting match 3972157\n",
      "Getting match 3972144\n",
      "Getting match 3972159\n",
      "Getting match 3939900\n",
      "Getting match 3972036\n",
      "Getting match 3972129\n",
      "Getting match 3972111\n",
      "Getting match 3972151\n",
      "Getting match 3972138\n",
      "Getting match 3968408\n",
      "Getting match 3939849\n",
      "Getting match 3972078\n",
      "Getting match 3939854\n",
      "Getting match 3972058\n",
      "Getting match 3939957\n",
      "Getting match 3972121\n",
      "Getting match 3972027\n",
      "Getting match 3939956\n",
      "Getting match 3939867\n",
      "Getting match 3939917\n",
      "Getting match 3972076\n",
      "Getting match 3939887\n",
      "Getting match 3972098\n",
      "Getting match 3972050\n",
      "Getting match 3939871\n",
      "Getting match 3972064\n",
      "Getting match 3939885\n",
      "Getting match 3972060\n",
      "Getting match 3939861\n",
      "Getting match 3939842\n",
      "Getting match 3972043\n",
      "Getting match 3939848\n",
      "Getting match 3939865\n",
      "Getting match 3939876\n",
      "Getting match 3939907\n",
      "Getting match 3969398\n",
      "Getting match 3939835\n",
      "Getting match 3972106\n",
      "Getting match 3972148\n",
      "Getting match 3939964\n",
      "Getting match 3939967\n",
      "Getting match 3972096\n",
      "Getting match 3972137\n",
      "Getting match 3939954\n",
      "Getting match 3939856\n",
      "Getting match 3972041\n",
      "Getting match 3972130\n",
      "Getting match 3972156\n",
      "Getting match 3972128\n",
      "Getting match 3972022\n",
      "Getting match 3972118\n",
      "Getting match 3939911\n",
      "Getting match 3972109\n",
      "Getting match 3939940\n",
      "Getting match 3972117\n",
      "Getting match 3972085\n",
      "Getting match 3939923\n",
      "Getting match 3993381\n",
      "Getting match 3972062\n",
      "Getting match 3939882\n",
      "Getting match 3939822\n",
      "Getting match 3939962\n",
      "Getting match 3939858\n",
      "Getting match 3972044\n",
      "Getting match 3939933\n",
      "Getting match 3939828\n",
      "Getting match 3972103\n",
      "Getting match 3939890\n",
      "Getting match 3972167\n",
      "Getting match 3972056\n",
      "Getting match 3939834\n",
      "Getting match 3972149\n",
      "Getting match 3972024\n",
      "Getting match 3972146\n",
      "Getting match 3972124\n",
      "Getting match 3972016\n",
      "Getting match 3972143\n",
      "Getting match 3972038\n",
      "Getting match 3939949\n",
      "Getting match 3972099\n",
      "Getting match 3972028\n",
      "Getting match 3939880\n",
      "Getting match 3972073\n",
      "Getting match 3972019\n",
      "Getting match 3994837\n",
      "Getting match 3972037\n",
      "Getting match 3939874\n",
      "Getting match 3939827\n",
      "Getting match 3939941\n",
      "Getting match 3993383\n",
      "Getting match 3939825\n",
      "Getting match 3939927\n",
      "Getting match 3993382\n",
      "Getting match 3972057\n",
      "Getting match 3939918\n",
      "Getting match 3972162\n",
      "Getting match 3939909\n",
      "Getting match 3972165\n",
      "Getting match 3972107\n",
      "Getting match 3939961\n",
      "Getting match 3939905\n",
      "Getting match 3972080\n",
      "Getting match 3939837\n",
      "Getting match 3939939\n",
      "Getting match 3970307\n",
      "Getting match 3993918\n",
      "Getting match 3972115\n",
      "Getting match 3939947\n",
      "Getting match 3972015\n",
      "Getting match 3939897\n",
      "Getting match 3939928\n",
      "Getting match 3972104\n",
      "Getting match 3939863\n",
      "Getting match 3995374\n",
      "Getting match 3972120\n",
      "Getting match 3939852\n",
      "Getting match 3939921\n",
      "Getting match 3939934\n",
      "Getting match 3993199\n",
      "Getting match 3939893\n",
      "Getting match 3939898\n",
      "Getting match 3939845\n",
      "Getting match 3939847\n",
      "Getting match 3939824\n",
      "Getting match 3939844\n",
      "Getting match 3972093\n",
      "Getting match 3939945\n",
      "Getting match 3992816\n",
      "Getting match 3972042\n",
      "Getting match 3972161\n",
      "Getting match 3972133\n",
      "Getting match 3939870\n",
      "Getting match 3972153\n",
      "Getting match 3939841\n",
      "Getting match 3939859\n",
      "Getting match 3972142\n",
      "Getting match 3970308\n",
      "Getting match 3939836\n",
      "Getting match 3972112\n",
      "Getting match 3939965\n",
      "Getting match 3939959\n",
      "Getting match 3972101\n",
      "Getting match 3972055\n",
      "Getting match 3939938\n",
      "Getting match 3972066\n",
      "Getting match 3972125\n",
      "Getting match 3969392\n",
      "Getting match 3939924\n",
      "Getting match 3972166\n",
      "Getting match 3972136\n",
      "Getting match 3972158\n",
      "Getting match 3972113\n",
      "Getting match 3972094\n",
      "Getting match 3972102\n",
      "Getting match 3972025\n",
      "Getting match 3972095\n",
      "Getting match 3972023\n",
      "Getting match 3969395\n",
      "Getting match 3939953\n",
      "Getting match 3939937\n",
      "Getting match 3939831\n",
      "Getting match 3939902\n",
      "Getting match 3972091\n",
      "Getting match 3972155\n",
      "Getting match 3939888\n",
      "Getting match 3939826\n",
      "Getting match 3939915\n",
      "Getting match 3972139\n",
      "Getting match 3939952\n",
      "Getting match 3995373\n",
      "Getting match 3939881\n",
      "Getting match 3972116\n",
      "Getting match 3939932\n",
      "Getting match 3972088\n",
      "Getting match 3993919\n",
      "Getting match 3972072\n",
      "Getting match 3972132\n",
      "Getting match 3972046\n",
      "Getting match 3993200\n",
      "Getting match 3972031\n",
      "Getting match 3972070\n",
      "Getting match 3939960\n",
      "Getting match 3939879\n",
      "Getting match 3939942\n",
      "Getting match 3972045\n",
      "Getting match 3939930\n",
      "Getting match 3972152\n",
      "Getting match 3939860\n",
      "Getting match 3972039\n",
      "Getting match 3939823\n",
      "Getting match 3939943\n",
      "Getting match 3972122\n",
      "Getting match 3939926\n",
      "Getting match 3972074\n",
      "Getting match 3939896\n",
      "Getting match 3972065\n",
      "Getting match 3939895\n",
      "Getting match 3971432\n",
      "Getting match 3939875\n",
      "Getting match 3939899\n",
      "Getting match 3939866\n",
      "Getting match 3972033\n",
      "Getting match 3972090\n",
      "Getting match 3969396\n",
      "Getting match 3972052\n",
      "Getting match 3968407\n",
      "Getting match 3972051\n",
      "Getting match 3939877\n",
      "Getting match 3972029\n",
      "Getting match 3972110\n",
      "Getting match 3972026\n",
      "Getting match 3972100\n",
      "Getting match 3972020\n",
      "Getting match 3939857\n",
      "Getting match 3939855\n",
      "Getting match 3939830\n",
      "Getting match 3939851\n",
      "Getting match 3972082\n",
      "Getting match 3939843\n",
      "Getting match 3972084\n",
      "Getting match 3939833\n",
      "Getting match 3972067\n",
      "Getting match 3939829\n",
      "Getting match 3969393\n",
      "Getting match 3939818\n",
      "Getting match 3939966\n",
      "Getting match 3972086\n",
      "Getting match 3939935\n",
      "Getting match 3972035\n",
      "Getting match 3939850\n",
      "Getting match 3972018\n",
      "Getting match 3939832\n",
      "Getting match 3939922\n",
      "Getting match 3972119\n",
      "Getting match 3939912\n",
      "Getting match 3939950\n",
      "Getting match 3939910\n",
      "Getting match 3939929\n",
      "Getting match 3994836\n",
      "Getting match 3939920\n",
      "Getting match 3972135\n",
      "Getting match 3939878\n",
      "Getting match 3939819\n",
      "Getting match 3939864\n",
      "Getting match 3972032\n",
      "Getting match 3939838\n",
      "Getting match 3969394\n",
      "Getting match 3939816\n",
      "Getting match 3939948\n",
      "Getting match 3972114\n",
      "Getting match 3972047\n",
      "Getting match 3972087\n",
      "Getting match 3972040\n",
      "Getting match 3969391\n",
      "Getting match 3972150\n",
      "Getting match 3939958\n",
      "Getting match 3972141\n",
      "Getting match 3939892\n",
      "Getting match 3970309\n",
      "Getting match 3939886\n",
      "Getting match 3939946\n",
      "Getting match 3939846\n",
      "Getting match 3939951\n",
      "Getting match 3970306\n",
      "Getting match 3939868\n",
      "Getting match 3972154\n",
      "Getting match 3939853\n",
      "Getting match 3972145\n",
      "Getting match 3939817\n",
      "Getting match 3972126\n",
      "Getting match 3972053\n",
      "Getting match 3972034\n",
      "Getting match 3939944\n",
      "Getting match 3971433\n",
      "Getting match 3972089\n",
      "Getting match 3972071\n",
      "Getting match 3972081\n",
      "Getting match 3993198\n",
      "Getting match 3972068\n",
      "Getting match 3972063\n",
      "Getting match 3972030\n",
      "Getting match 3939891\n",
      "Getting match 3939968\n",
      "Getting match 3939919\n",
      "Getting match 3939936\n",
      "Getting match 3972160\n",
      "Getting match 3939931\n",
      "Getting match 3994838\n",
      "Getting match 3939925\n",
      "Getting match 3972108\n",
      "Getting match 3939889\n",
      "Getting match 3939862\n",
      "Getting match 3939839\n",
      "Getting match 3939840\n",
      "Getting match 3972079\n",
      "Getting match 3939955\n",
      "Getting match 3972134\n",
      "Getting match 3972105\n",
      "Getting match 3972097\n",
      "Getting match 3972077\n",
      "Getting match 3972069\n",
      "Getting match 3939913\n",
      "Getting match 3939873\n",
      "Getting match 3972061\n",
      "Getting match 3969397\n",
      "Getting match 3939869\n",
      "Getting match 3939820\n"
     ]
    }
   ],
   "source": [
    "get_players = True\n",
    "if get_players:\n",
    "    df_players = build_unique_players_simple(matches_all=matches_all, email=email, password=password)\n",
    "    df_players.head()\n",
    "    df_players.to_parquet('./artifacts/players_unique.parquet', index=False)\n",
    "else:\n",
    "    df_players = pd.read_parquet('./artifacts/players_unique.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "105b4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data = False\n",
    "if get_data:\n",
    "    events_2122 = sb.competition_events(\n",
    "        country=\"Mexico\",\n",
    "        division= \"Liga MX\",\n",
    "        season=\"2021/2022\",\n",
    "        gender=\"male\",\n",
    "        creds={\"user\": email, \"passwd\": password}\n",
    "    )\n",
    "    events_2223 = sb.competition_events(\n",
    "        country=\"Mexico\",\n",
    "        division= \"Liga MX\",\n",
    "        season=\"2022/2023\",\n",
    "        gender=\"male\",\n",
    "        creds={\"user\": email, \"passwd\": password}\n",
    "    )\n",
    "    events_2324 = sb.competition_events(\n",
    "        country=\"Mexico\",\n",
    "        division= \"Liga MX\",\n",
    "        season=\"2023/2024\",\n",
    "        gender=\"male\",\n",
    "        creds={\"user\": email, \"passwd\": password}\n",
    "    )\n",
    "    events_2425 = sb.competition_events(\n",
    "        country=\"Mexico\",\n",
    "        division= \"Liga MX\",\n",
    "        season=\"2024/2025\",\n",
    "        gender=\"male\",\n",
    "        creds={\"user\": email, \"passwd\": password}\n",
    "    )\n",
    "    events_2122.to_csv('data/events_2122.csv')\n",
    "    events_2223.to_csv('data/events_2223.csv')\n",
    "    events_2324.to_csv('data/events_2324.csv')\n",
    "    events_2425.to_csv('data/events_2425.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "834ddde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/1999902261.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,47,48,52,60,61,62,64,65,66,67,68,69,70,71,72,74,75,76,77,79,82,84,85,86,87,88,89,95,102,103,104,105,106,107,108,112,113,114,115,116,117,118,121,122,124,125,126,128,130,135) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events_2122 = pd.read_csv('data/events_2122.csv')\n",
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/1999902261.py:2: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,46,47,51,59,60,61,63,64,65,66,67,68,69,70,71,73,74,75,76,78,81,83,84,85,86,87,88,94,101,102,103,104,105,106,107,111,112,113,114,115,116,117,120,121,123,124,125,127,129,134) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events_2223 = pd.read_csv('data/events_2223.csv')\n",
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/1999902261.py:3: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,47,48,52,60,61,62,64,65,66,67,68,69,70,71,72,74,75,76,77,79,82,84,85,86,87,88,89,95,102,103,104,105,106,107,108,112,113,114,115,116,117,118,121,122,124,125,126,128,130,135) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events_2324 = pd.read_csv('data/events_2324.csv')\n",
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/1999902261.py:4: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,46,47,51,59,60,61,63,64,65,66,67,68,69,70,71,73,74,75,76,78,81,83,84,85,86,87,88,94,101,102,103,104,105,106,107,111,112,113,114,115,116,117,120,121,123,124,125,127,129,134) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events_2425 = pd.read_csv('data/events_2425.csv')\n"
     ]
    }
   ],
   "source": [
    "events_2122 = pd.read_csv('data/events_2122.csv')\n",
    "events_2223 = pd.read_csv('data/events_2223.csv')\n",
    "events_2324 = pd.read_csv('data/events_2324.csv')\n",
    "events_2425 = pd.read_csv('data/events_2425.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d0277ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a \"season\" column to each events df\n",
    "events_2122['season'] = \"2021/2022\"\n",
    "events_2223['season'] = \"2022/2023\"\n",
    "events_2324['season'] = \"2023/2024\"\n",
    "events_2425['season'] = \"2024/2025\"\n",
    "\n",
    "# Concatenate all events into a single df\n",
    "events_all = pd.concat([events_2122, events_2223, events_2324, events_2425], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a091fda",
   "metadata": {},
   "source": [
    "### 1. Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "084bc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data = True\n",
    "if get_data:\n",
    "    data_all = pd.merge(events_all, matches_all, 'left', on = ['match_id', 'season'])\n",
    "    data_all.to_csv('data/data_all.csv')\n",
    "else:\n",
    "    data_all = pd.read_csv('data/data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf9d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "# =========================================================\n",
    "# 0) Utilidades de coordenadas (StatsBomb-like 120x80)\n",
    "# =========================================================\n",
    "def _extract_xyz(val) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    if isinstance(val, (list, tuple)) and len(val) >= 2:\n",
    "        x = float(val[0]) if val[0] is not None else None\n",
    "        y = float(val[1]) if val[1] is not None else None\n",
    "        z = float(val[2]) if (len(val) >= 3 and val[2] is not None) else None\n",
    "        return x, y, z\n",
    "    return None, None, None\n",
    "\n",
    "def add_xyz_columns(df: pd.DataFrame, src_col: str, prefix: Optional[str] = None) -> pd.DataFrame:\n",
    "    if src_col not in df.columns:\n",
    "        return df\n",
    "    if prefix is None:\n",
    "        prefix = (src_col\n",
    "                  .replace(\"_end_location\", \"_end\")\n",
    "                  .replace(\"location\", \"loc\")\n",
    "                  .replace(\"__\", \"_\"))\n",
    "    xs, ys, zs = zip(*df[src_col].apply(_extract_xyz))\n",
    "    df[f\"{prefix}_x\"] = xs\n",
    "    df[f\"{prefix}_y\"] = ys\n",
    "    df[f\"{prefix}_z\"] = zs\n",
    "    return df\n",
    "\n",
    "def add_xy_all(data_all: pd.DataFrame, extra_end_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    df = data_all.copy()\n",
    "    location_cols = [\n",
    "        \"location\",\n",
    "        \"pass_end_location\",\n",
    "        \"shot_end_location\",\n",
    "        \"carry_end_location\",\n",
    "        \"goalkeeper_end_location\",\n",
    "    ]\n",
    "    if extra_end_cols:\n",
    "        location_cols.extend(extra_end_cols)\n",
    "    for c in location_cols:\n",
    "        if c in df.columns:\n",
    "            pref = (c.replace(\"_end_location\", \"_end\")\n",
    "                      .replace(\"location\", \"loc\")\n",
    "                      .replace(\"__\", \"_\"))\n",
    "            add_xyz_columns(df, c, prefix=pref)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_pitch_units(df_with_xy: pd.DataFrame,\n",
    "                        in_dims: Tuple[float, float]=(120.0, 80.0),\n",
    "                        out_dims: Tuple[float, float]=(105.0, 68.0),\n",
    "                        prefixes: Optional[List[str]]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert StatsBomb coordinates (120x80) to meters (default 105x68).\n",
    "    Scales all '*_x'/'*_y' columns matching given prefixes; if prefixes=None, auto-detects.\n",
    "    \"\"\"\n",
    "    df = df_with_xy.copy()\n",
    "    in_x, in_y = in_dims\n",
    "    out_x, out_y = out_dims\n",
    "    sx = out_x / in_x\n",
    "    sy = out_y / in_y\n",
    "\n",
    "    if prefixes is None:\n",
    "        prefixes = sorted(set(c[:-2] for c in df.columns if c.endswith(\"_x\")))\n",
    "\n",
    "    for pref in prefixes:\n",
    "        xcol, ycol = f\"{pref}_x\", f\"{pref}_y\"\n",
    "        if xcol in df.columns:\n",
    "            df[xcol] = pd.to_numeric(df[xcol], errors=\"coerce\") * sx\n",
    "        if ycol in df.columns:\n",
    "            df[ycol] = pd.to_numeric(df[ycol], errors=\"coerce\") * sy\n",
    "    return df\n",
    "\n",
    "# =========================================================\n",
    "# 1) Season normalization & helpers\n",
    "# =========================================================\n",
    "def _ensure_season(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure a 'season' column exists. If missing/empty, derive from year(match_date).\n",
    "    Accepts match_date as 'YYYY-MM-DD' or parseable timestamp-like.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if \"season\" not in out.columns or out[\"season\"].isna().all():\n",
    "        if \"match_date\" in out.columns:\n",
    "            years = pd.to_datetime(out[\"match_date\"], errors=\"coerce\").dt.year.astype(\"Int64\")\n",
    "            out[\"season\"] = years.astype(str)\n",
    "        else:\n",
    "            out[\"season\"] = \"unknown\"\n",
    "    return out\n",
    "\n",
    "def _safe_bool_series(s: pd.Series, default: bool=False) -> pd.Series:\n",
    "    \"\"\"Coerce arbitrary series into a boolean series safely.\"\"\"\n",
    "    if s is None:\n",
    "        return pd.Series([default]*0, dtype=bool)\n",
    "    if s.dtype == bool:\n",
    "        return s.fillna(default)\n",
    "    return s.astype(str).str.lower().isin([\"true\",\"t\",\"1\",\"yes\",\"y\"]).fillna(default)\n",
    "\n",
    "# =========================================================\n",
    "# 2) Minutes played (match-level) heuristic without lineups\n",
    "# =========================================================\n",
    "def estimate_minutes_by_match_player(data_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Heuristic using events only (no lineups):\n",
    "      - For each match: duration ~ max(minute + second/60), clipped to [0,130].\n",
    "      - For each player in that match:\n",
    "            start_min = min event minute for that player\n",
    "            end_min   = first event with player_off_permanent==True OR match_duration\n",
    "            minutes   = max(0, end_min - start_min)\n",
    "      - If minutes = 0 but player appeared, set to 1.0 to avoid divide-by-zero later.\n",
    "    Returns: match_id, season, player_id, player, team, minutes_played\n",
    "    \"\"\"\n",
    "    df = _ensure_season(data_all).copy()\n",
    "\n",
    "    minute = pd.to_numeric(df.get(\"minute\", np.nan), errors=\"coerce\")\n",
    "    second = pd.to_numeric(df.get(\"second\", 0), errors=\"coerce\").fillna(0)\n",
    "    tmin = minute.fillna(0) + (second.fillna(0) / 60.0)\n",
    "    df[\"_tmin\"] = tmin\n",
    "\n",
    "    match_dur = df.groupby(\"match_id\")[\"_tmin\"].max().clip(lower=0, upper=130).rename(\"match_duration_min\")\n",
    "    start_min = df.groupby([\"match_id\",\"player_id\"])[\"_tmin\"].min().rename(\"start_min\")\n",
    "\n",
    "    off_perm = _safe_bool_series(df.get(\"player_off_permanent\", pd.Series(False, index=df.index)), default=False)\n",
    "    df[\"_off_perm\"] = off_perm\n",
    "    off_times = df.loc[df[\"_off_perm\"]].groupby([\"match_id\",\"player_id\"])[\"_tmin\"].min().rename(\"off_min\")\n",
    "\n",
    "    mp = (start_min.to_frame()\n",
    "                 .join(off_times, how=\"left\")\n",
    "                 .join(match_dur, on=\"match_id\", how=\"left\")\n",
    "                 .reset_index())\n",
    "\n",
    "    mp[\"end_min\"] = np.where(mp[\"off_min\"].notna(), mp[\"off_min\"], mp[\"match_duration_min\"])\n",
    "    mp[\"minutes_played\"] = (mp[\"end_min\"] - mp[\"start_min\"]).clip(lower=0)\n",
    "    mp.loc[(mp[\"minutes_played\"] == 0) & mp[\"start_min\"].notna(), \"minutes_played\"] = 1.0\n",
    "\n",
    "    meta_cols = [\"player_id\",\"player\",\"team\",\"season\",\"match_id\"]\n",
    "    meta = (_ensure_season(df)[meta_cols]\n",
    "            .drop_duplicates(subset=[\"match_id\",\"player_id\"])\n",
    "            .set_index([\"match_id\",\"player_id\"]))\n",
    "    mp = (mp.set_index([\"match_id\",\"player_id\"])\n",
    "            .join(meta, how=\"left\")\n",
    "            .reset_index())\n",
    "\n",
    "    mp = mp[[\"match_id\",\"season\",\"player_id\",\"player\",\"team\",\"minutes_played\"]].fillna({\"player\":\"\", \"team\":\"\"})\n",
    "    return mp\n",
    "\n",
    "# =========================================================\n",
    "# 3) Season-team-player aggregations (totals)\n",
    "# =========================================================\n",
    "def aggregate_player_team_season_counts(data_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate by (season, team, player_id, player):\n",
    "      shots, goals, xg, passes_att, passes_cmp, pass_pct, key_passes, assists, xA,\n",
    "      dribbles_cmp, duels_won, interceptions, blocks, clearances,\n",
    "      obv_total, obv_for, obv_against\n",
    "    \"\"\"\n",
    "    df = _ensure_season(data_all).copy()\n",
    "    for col in [\"type\", \"player\", \"player_id\", \"team\", \"season\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    grp = [\"season\",\"team\",\"player_id\",\"player\"]\n",
    "\n",
    "    # flags (0/1) to avoid Series vs DataFrame groupby issues\n",
    "    df[\"is_shot\"] = (df[\"type\"] == \"Shot\").astype(int)\n",
    "    df[\"is_goal\"] = ((df[\"type\"] == \"Shot\") & (df.get(\"shot_outcome\", pd.Series([None]*len(df))) == \"Goal\")).astype(int)\n",
    "\n",
    "    xg_col = \"shot_statsbomb_xg\"\n",
    "    df[\"_xg\"] = np.where(df[\"type\"].eq(\"Shot\"), df.get(xg_col, np.nan), np.nan)\n",
    "\n",
    "    df[\"is_pass\"] = (df[\"type\"] == \"Pass\").astype(int)\n",
    "    pass_outcome = df.get(\"pass_outcome\", pd.Series([np.nan]*len(df)))\n",
    "    df[\"is_pass_cmp\"] = ((df[\"type\"] == \"Pass\") & (pass_outcome.isna())).astype(int)\n",
    "\n",
    "    df[\"is_key_pass\"] = (df.get(\"pass_shot_assist\", False).fillna(False) & df[\"type\"].eq(\"Pass\")).astype(int)\n",
    "    df[\"is_assist\"]   = (df.get(\"pass_goal_assist\", False).fillna(False) & df[\"type\"].eq(\"Pass\")).astype(int)\n",
    "\n",
    "    dribble_outcome = df.get(\"dribble_outcome\", pd.Series([np.nan]*len(df))).astype(str).str.lower()\n",
    "    df[\"is_dribble_cmp\"] = ((df[\"type\"] == \"Dribble\") & (dribble_outcome.str.contains(\"complete\"))).astype(int)\n",
    "\n",
    "    duel_outcome = df.get(\"duel_outcome\", pd.Series([np.nan]*len(df))).astype(str).str.lower()\n",
    "    df[\"is_duel_won\"] = ((df[\"type\"] == \"Duel\") & (duel_outcome.str.contains(\"won\"))).astype(int)\n",
    "\n",
    "    inter_outcome = df.get(\"interception_outcome\", pd.Series([np.nan]*len(df))).astype(str).str.lower()\n",
    "    df[\"is_interception\"] = ((df[\"type\"] == \"Interception\") & (inter_outcome.str.contains(\"won|success\"))).astype(int)\n",
    "\n",
    "    block_cols = [c for c in df.columns if c.startswith(\"block_\")]\n",
    "    if block_cols:\n",
    "        blocks_any = df[block_cols].fillna(False).any(axis=1)\n",
    "        df[\"is_block\"] = (blocks_any & df[\"type\"].eq(\"Block\")).astype(int)\n",
    "    else:\n",
    "        df[\"is_block\"] = 0\n",
    "\n",
    "    if \"type\" in df.columns and df[\"type\"].eq(\"Clearance\").any():\n",
    "        df[\"is_clearance\"] = df[\"type\"].eq(\"Clearance\").astype(int)\n",
    "    else:\n",
    "        clr_cols = [c for c in df.columns if c.startswith(\"clearance_\")]\n",
    "        if clr_cols:\n",
    "            cl_any = df[clr_cols].fillna(False).any(axis=1)\n",
    "            df[\"is_clearance\"] = cl_any.astype(int)\n",
    "        else:\n",
    "            df[\"is_clearance\"] = 0\n",
    "\n",
    "    # grouped totals\n",
    "    shots          = df.groupby(grp)[\"is_shot\"].sum(min_count=1).rename(\"shots\")\n",
    "    goals          = df.groupby(grp)[\"is_goal\"].sum(min_count=1).rename(\"goals\")\n",
    "    xg             = df.groupby(grp)[\"_xg\"].sum(min_count=1).rename(\"xg\")\n",
    "\n",
    "    passes_att     = df.groupby(grp)[\"is_pass\"].sum(min_count=1).rename(\"passes_att\")\n",
    "    passes_cmp     = df.groupby(grp)[\"is_pass_cmp\"].sum(min_count=1).rename(\"passes_cmp\")\n",
    "    key_passes     = df.groupby(grp)[\"is_key_pass\"].sum(min_count=1).rename(\"key_passes\")\n",
    "    assists        = df.groupby(grp)[\"is_assist\"].sum(min_count=1).rename(\"assists\")\n",
    "    dribbles_cmp   = df.groupby(grp)[\"is_dribble_cmp\"].sum(min_count=1).rename(\"dribbles_cmp\")\n",
    "    duels_won      = df.groupby(grp)[\"is_duel_won\"].sum(min_count=1).rename(\"duels_won\")\n",
    "    interceptions  = df.groupby(grp)[\"is_interception\"].sum(min_count=1).rename(\"interceptions\")\n",
    "    blocks         = df.groupby(grp)[\"is_block\"].sum(min_count=1).rename(\"blocks\")\n",
    "    clearances     = df.groupby(grp)[\"is_clearance\"].sum(min_count=1).rename(\"clearances\")\n",
    "\n",
    "    # OBV\n",
    "    obv_total   = df.get(\"obv_total_net\", pd.Series([np.nan]*len(df)))\n",
    "    obv_for     = df.get(\"obv_for_net\",   pd.Series([np.nan]*len(df)))\n",
    "    obv_against = df.get(\"obv_against_net\", pd.Series([np.nan]*len(df)))\n",
    "\n",
    "    obv_total_g    = df.assign(_obv_total=obv_total).groupby(grp)[\"_obv_total\"].sum(min_count=1).rename(\"obv_total\")\n",
    "    obv_for_g      = df.assign(_obv_for=obv_for).groupby(grp)[\"_obv_for\"].sum(min_count=1).rename(\"obv_for\")\n",
    "    obv_against_g  = df.assign(_obv_against=obv_against).groupby(grp)[\"_obv_against\"].sum(min_count=1).rename(\"obv_against\")\n",
    "\n",
    "    # xA (needs shot_key_pass_id, id, xg_col)\n",
    "    xA = pd.Series(dtype=float, name=\"xA\")\n",
    "    if {\"shot_key_pass_id\",\"id\",xg_col}.issubset(df.columns):\n",
    "        shots_kp  = df.loc[df[\"type\"].eq(\"Shot\"), [\"id\",\"shot_key_pass_id\",xg_col,\"season\"]].dropna(subset=[\"shot_key_pass_id\"])\n",
    "        passes_kp = df.loc[df[\"type\"].eq(\"Pass\"), [\"id\",\"player_id\",\"player\",\"team\",\"season\"]]\n",
    "        xA_join   = shots_kp.merge(passes_kp, left_on=[\"shot_key_pass_id\",\"season\"], right_on=[\"id\",\"season\"], how=\"left\")\n",
    "        xA        = xA_join.groupby([\"season\",\"team\",\"player_id\",\"player\"])[xg_col].sum(min_count=1).rename(\"xA\")\n",
    "\n",
    "    pieces = [shots, goals, xg, passes_att, passes_cmp, key_passes, assists,\n",
    "              dribbles_cmp, duels_won, interceptions, blocks, clearances,\n",
    "              obv_total_g, obv_for_g, obv_against_g]\n",
    "    if not xA.empty:\n",
    "        pieces.append(xA)\n",
    "\n",
    "    agg = pd.concat(pieces, axis=1).fillna(0)\n",
    "\n",
    "    # pass%\n",
    "    agg[\"pass_pct\"] = np.where(agg[\"passes_att\"] > 0,\n",
    "                               100.0 * agg[\"passes_cmp\"] / agg[\"passes_att\"],\n",
    "                               np.nan)\n",
    "\n",
    "    # nice dtypes for counts\n",
    "    for c in [\"shots\",\"goals\",\"passes_att\",\"passes_cmp\",\"key_passes\",\"assists\",\n",
    "              \"dribbles_cmp\",\"duels_won\",\"interceptions\",\"blocks\",\"clearances\"]:\n",
    "        if c in agg.columns:\n",
    "            agg[c] = agg[c].astype(\"Int64\")\n",
    "\n",
    "    return agg.reset_index().sort_values([\"season\",\"team\",\"player\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================================================\n",
    "# 4) Minutes & games per season-team-player; per90s\n",
    "# =========================================================\n",
    "def player_team_season_minutes_games(data_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns minutes and games by (season, team, player_id, player):\n",
    "      minutes = sum of estimated minutes_played\n",
    "      games   = #matches with minutes_played > 0\n",
    "    \"\"\"\n",
    "    mp = estimate_minutes_by_match_player(data_all)\n",
    "    grp = [\"season\",\"team\",\"player_id\",\"player\"]\n",
    "\n",
    "    minutes = mp.groupby(grp)[\"minutes_played\"].sum(min_count=1).rename(\"minutes\")\n",
    "    mp[\"played_flag\"] = (mp[\"minutes_played\"] > 0).astype(int)\n",
    "    games = mp.groupby(grp)[\"played_flag\"].sum(min_count=1).rename(\"games\").astype(\"Int64\")\n",
    "\n",
    "    out = pd.concat([minutes, games], axis=1).reset_index()\n",
    "    out[[\"minutes\"]] = out[[\"minutes\"]].fillna(0.0)\n",
    "    out[[\"games\"]]   = out[[\"games\"]].fillna(0)\n",
    "    return out\n",
    "\n",
    "def add_per90(agg_counts: pd.DataFrame, minutes_games: pd.DataFrame,\n",
    "              per90_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge totals with minutes/games and compute per90 for selected columns.\n",
    "    \"\"\"\n",
    "    if per90_cols is None:\n",
    "        per90_cols = [\n",
    "            \"shots\",\"goals\",\"xg\",\"passes_att\",\"passes_cmp\",\"key_passes\",\"assists\",\"xA\",\n",
    "            \"dribbles_cmp\",\"duels_won\",\"interceptions\",\"blocks\",\"clearances\",\n",
    "            \"obv_total\",\"obv_for\",\"obv_against\"\n",
    "        ]\n",
    "    df = agg_counts.merge(minutes_games, on=[\"season\",\"team\",\"player_id\",\"player\"], how=\"left\")\n",
    "    df[\"minutes\"] = df[\"minutes\"].fillna(0.0)\n",
    "    denom = df[\"minutes\"].replace(0, np.nan) / 90.0\n",
    "    for c in per90_cols:\n",
    "        if c in df.columns:\n",
    "            df[f\"{c}_per90\"] = df[c] / denom\n",
    "    return df\n",
    "\n",
    "# =========================================================\n",
    "# 5) Full pipeline\n",
    "# =========================================================\n",
    "def build_player_team_season_table(data_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      1) (Optional) add_xy_all(data_all)  # not required for aggregations\n",
    "      2) Aggregate season-team-player totals\n",
    "      3) Estimate minutes & games season-team-player\n",
    "      4) Compute per90s\n",
    "    Returns one row per (season, team, player_id, player).\n",
    "    \"\"\"\n",
    "    agg = aggregate_player_team_season_counts(data_all)\n",
    "    mg  = player_team_season_minutes_games(data_all)\n",
    "    out = add_per90(agg, mg)\n",
    "    # Friendly column order\n",
    "    first = [\"season\",\"team\",\"player_id\",\"player\",\"games\",\"minutes\"]\n",
    "    others = [c for c in out.columns if c not in first]\n",
    "    return out[first + others]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611c7bd",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b5b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= EXPORT PIPELINE (TEAM-AGNOSTIC) =========================\n",
    "# Drop this in a notebook cell or a .py file and run export_artifacts(...)\n",
    "# Requires: pandas, numpy, scikit-learn\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b500a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# A) Recency-weighted per-90 (team-agnostic)\n",
    "# ------------------------------------------------------------------------------\n",
    "def _detect_per90_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return sorted([c for c in df.columns if c.endswith(\"_per90\")])\n",
    "\n",
    "def _season_sort_key(s):\n",
    "    try:\n",
    "        return int(str(s).split(\"-\")[0])\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "def recency_weights_k4() -> List[float]:\n",
    "    return [0.05, 0.10, 0.25, 0.60]  # oldest -> newest\n",
    "\n",
    "def _effective_weights(weights: Sequence[float], n: int) -> np.ndarray:\n",
    "    w = np.array(weights[-n:], dtype=float)\n",
    "    return w / w.sum() if w.sum() > 0 else np.ones(n)/n\n",
    "\n",
    "def _minutes_scaler(minutes: pd.Series, floor: float = 450.0, cap: float = 2700.0, power: float = 0.5) -> np.ndarray:\n",
    "    m = minutes.fillna(0).clip(lower=0, upper=cap).astype(float).values\n",
    "    z = np.clip(m / floor, 0.0, cap / floor)\n",
    "    return np.power(z, power)\n",
    "\n",
    "def build_recency_weighted_per90_team_agnostic(\n",
    "    seasonal_table: pd.DataFrame,\n",
    "    keys: List[str] = [\"player_id\",\"player\"],\n",
    "    K: int = 4,\n",
    "    recency_weights: Optional[Sequence[float]] = None,\n",
    "    use_minutes_scaler: bool = True,\n",
    "    minutes_col: str = \"minutes\",\n",
    "    suffix: str = \"_per90_rw\"\n",
    ") -> pd.DataFrame:\n",
    "    if recency_weights is None:\n",
    "        recency_weights = recency_weights_k4()\n",
    "    per90_cols = _detect_per90_cols(seasonal_table)\n",
    "    if not per90_cols:\n",
    "        raise ValueError(\"No *_per90 columns found in seasonal_table.\")\n",
    "    req = set(keys + [\"season\", minutes_col])\n",
    "    miss = req - set(seasonal_table.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"seasonal_table missing: {miss}\")\n",
    "\n",
    "    df = seasonal_table.copy()\n",
    "    df[\"_season_key\"] = df[\"season\"].apply(_season_sort_key)\n",
    "\n",
    "    out = []\n",
    "    for gvals, gdf in df.groupby(keys, dropna=False):\n",
    "        gdf = gdf.sort_values(\"_season_key\").tail(K)\n",
    "        if not len(gdf):\n",
    "            continue\n",
    "        base_w = _effective_weights(recency_weights, len(gdf))\n",
    "        if use_minutes_scaler:\n",
    "            msc = _minutes_scaler(gdf[minutes_col])\n",
    "            w = base_w * msc\n",
    "            w = w / w.sum() if w.sum() > 0 else base_w\n",
    "        else:\n",
    "            w = base_w\n",
    "\n",
    "        row = dict(zip(keys, gvals if isinstance(gvals, tuple) else (gvals,)))\n",
    "        for c in per90_cols:\n",
    "            v = gdf[c].astype(float).values\n",
    "            mask = ~np.isnan(v)\n",
    "            if mask.sum() == 0:\n",
    "                row[c.replace(\"_per90\", suffix)] = np.nan\n",
    "            else:\n",
    "                ww = w[mask]; ww = ww/ww.sum() if ww.sum()>0 else np.ones(mask.sum())/mask.sum()\n",
    "                row[c.replace(\"_per90\", suffix)] = float(np.dot(v[mask], ww))\n",
    "        row[\"seasons_used\"] = \"|\".join(map(str, gdf[\"season\"].tolist()))\n",
    "        row[\"minutes_sum\"] = float(gdf[minutes_col].fillna(0).sum())\n",
    "        row[\"games_sum\"] = int(gdf[\"games\"].fillna(0).sum()) if \"games\" in gdf.columns else None\n",
    "        row[\"last_season\"] = str(gdf[\"season\"].iloc[-1])\n",
    "        out.append(row)\n",
    "    return pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5929f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# B) Position grouping & top-3 positions (≥ 10 distinct matches) — team-agnostic\n",
    "# ------------------------------------------------------------------------------\n",
    "POSITION_GROUP_MAP = {\n",
    "    # Fullbacks & Wingbacks\n",
    "    \"Right Back\": \"Right Back\", \"Right Wing Back\": \"Right Back\",\n",
    "    \"Left Back\": \"Left Back\",   \"Left Wing Back\": \"Left Back\",\n",
    "    # Center Backs\n",
    "    \"Center Back\": \"Center Back\", \"Right Center Back\": \"Center Back\", \"Left Center Back\": \"Center Back\",\n",
    "    # DMs\n",
    "    \"Center Defensive Midfield\": \"Defensive Midfield\",\n",
    "    \"Right Defensive Midfield\": \"Defensive Midfield\",\n",
    "    \"Left Defensive Midfield\":  \"Defensive Midfield\",\n",
    "    # CMs\n",
    "    \"Center Midfield\": \"Center Midfield\",\n",
    "    \"Right Center Midfield\": \"Center Midfield\",\n",
    "    \"Left Center Midfield\":  \"Center Midfield\",\n",
    "    # AMs\n",
    "    \"Center Attacking Midfield\": \"Attacking Midfield\",\n",
    "    \"Right Attacking Midfield\":  \"Attacking Midfield\",\n",
    "    \"Left Attacking Midfield\":   \"Attacking Midfield\",\n",
    "    # Wingers / wide mids\n",
    "    \"Right Wing\": \"Right Wing\", \"Right Midfield\": \"Right Wing\",\n",
    "    \"Left Wing\":  \"Left Wing\",  \"Left Midfield\":  \"Left Wing\",\n",
    "    # Forwards\n",
    "    \"Center Forward\": \"Center Forward\",\n",
    "    \"Right Center Forward\": \"Center Forward\",\n",
    "    \"Left Center Forward\":  \"Center Forward\",\n",
    "    # Others\n",
    "    \"Goalkeeper\": \"Goalkeeper\",\n",
    "    \"Substitute\": \"Substitute\",\n",
    "}\n",
    "ALLOWED_ORIGINAL = set(PERSON for PERSON in POSITION_GROUP_MAP.keys())\n",
    "\n",
    "def top_positions_grouped_team_agnostic(\n",
    "    events: pd.DataFrame,\n",
    "    keys: List[str] = [\"player_id\",\"player\"],\n",
    "    pos_col: str = \"position\",\n",
    "    match_col: str = \"match_id\",\n",
    "    date_col: str = \"match_date\",\n",
    "    min_games: int = 10,\n",
    "    top_k: int = 3,\n",
    "    exclude_substitute: bool = True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    req = set(keys + [pos_col, match_col])\n",
    "    missing = req - set(events.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"events missing: {missing}\")\n",
    "\n",
    "    df = events[keys + [pos_col, match_col] + ([date_col] if date_col in events.columns else [])].copy()\n",
    "    df = df.dropna(subset=[pos_col, match_col])\n",
    "    df = df[df[pos_col].isin(ALLOWED_ORIGINAL)]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows with allowed positions.\")\n",
    "    df[\"grouped_pos\"] = df[pos_col].map(POSITION_GROUP_MAP)\n",
    "    if exclude_substitute:\n",
    "        df = df[df[\"grouped_pos\"] != \"Substitute\"]\n",
    "\n",
    "    dedup = df.drop_duplicates(subset=keys + [\"grouped_pos\", match_col])\n",
    "    games = (dedup.groupby(keys + [\"grouped_pos\"], as_index=False)[match_col]\n",
    "                  .nunique().rename(columns={match_col: \"games\"}))\n",
    "\n",
    "    if date_col in df.columns:\n",
    "        tmp = df.copy()\n",
    "        tmp[date_col] = pd.to_datetime(tmp[date_col], errors=\"coerce\")\n",
    "        last_date = (tmp.dropna(subset=[date_col])\n",
    "                       .groupby(keys + [\"grouped_pos\"], as_index=False)[date_col]\n",
    "                       .max())\n",
    "        summary = games.merge(last_date, on=keys + [\"grouped_pos\"], how=\"left\")\n",
    "    else:\n",
    "        summary = games.copy()\n",
    "        summary[date_col] = pd.NaT\n",
    "\n",
    "    summary = summary[summary[\"games\"] >= min_games]\n",
    "    if summary.empty:\n",
    "        wide_cols = keys + [\"primary_pos\",\"primary_games\",\"secondary_pos\",\"secondary_games\",\"third_pos\",\"third_games\"]\n",
    "        return pd.DataFrame(columns=wide_cols), summary\n",
    "\n",
    "    def topk_rows(g: pd.DataFrame) -> pd.Series:\n",
    "        g2 = g.sort_values([\"games\", date_col], ascending=[False, False])\n",
    "        pos_list  = g2[\"grouped_pos\"].tolist()[:top_k]\n",
    "        game_list = g2[\"games\"].tolist()[:top_k]\n",
    "        pos_list  += [None] * (top_k - len(pos_list))\n",
    "        game_list += [np.nan] * (top_k - len(game_list))\n",
    "        names = [\"primary\",\"secondary\",\"third\"][:top_k]\n",
    "        out = {}\n",
    "        for i, nm in enumerate(names):\n",
    "            out[f\"{nm}_pos\"]   = pos_list[i]\n",
    "            out[f\"{nm}_games\"] = game_list[i]\n",
    "        return pd.Series(out)\n",
    "\n",
    "    wide = (summary.groupby(keys, as_index=False)\n",
    "                  .apply(topk_rows)\n",
    "                  .reset_index()\n",
    "                  .drop(columns=[\"level_0\"], errors=\"ignore\"))\n",
    "    wide = wide.reindex(columns=keys + [\"primary_pos\",\"primary_games\",\"secondary_pos\",\"secondary_games\",\"third_pos\",\"third_games\"])\n",
    "    long_df = summary.rename(columns={date_col: \"last_date\"})[keys + [\"grouped_pos\",\"games\",\"last_date\"]]\n",
    "    return wide, long_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a70ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# C) Latest team per player (team-agnostic label) & events_min\n",
    "# ------------------------------------------------------------------------------\n",
    "def latest_team_per_player(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp = events[[\"player_id\",\"player\",\"team\",\"match_date\"]].dropna(subset=[\"player_id\",\"team\"]).copy()\n",
    "    tmp[\"match_date\"] = pd.to_datetime(tmp[\"match_date\"], errors=\"coerce\")\n",
    "    tmp = tmp.sort_values(\"match_date\").dropna(subset=[\"match_date\"])\n",
    "    lt = tmp.groupby([\"player_id\",\"player\"], as_index=False).last()\n",
    "    return lt.rename(columns={\"team\":\"latest_team\"})[[\"player_id\",\"player\",\"latest_team\"]]\n",
    "\n",
    "def build_events_min(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep = [\"match_id\",\"match_date\",\"team\",\"team_id\",\"player_id\",\"player\",\"type\",\n",
    "            \"minute\",\"second\",\"tactics\",\"home_team\",\"away_team\",\"shot_statsbomb_xg\"]\n",
    "    cols = [c for c in keep if c in events.columns]\n",
    "    return events[cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57a3c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# D) Clustering within grouped positions (auto-K ≤ 5)\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Map any synonyms/variants to your 10 canonical grouped positions\n",
    "POSITION_NORMALIZER = {\n",
    "    # full-backs / wing-backs\n",
    "    \"Right Wing Back\": \"Right Back\",\n",
    "    \"Left Wing Back\":  \"Left Back\",\n",
    "    \"RB\": \"Right Back\",\n",
    "    \"LB\": \"Left Back\",\n",
    "\n",
    "    # center back synonyms\n",
    "    \"RCB\": \"Center Back\",\n",
    "    \"LCB\": \"Center Back\",\n",
    "    \"CB\":  \"Center Back\",\n",
    "\n",
    "    # midfield\n",
    "    \"CDM\": \"Defensive Midfield\",\n",
    "    \"DM\":  \"Defensive Midfield\",\n",
    "    \"CM\":  \"Center Midfield\",\n",
    "    \"CAM\": \"Attacking Midfield\",\n",
    "    \"AM\":  \"Attacking Midfield\",\n",
    "\n",
    "    # wide\n",
    "    \"RW\": \"Right Wing\",\n",
    "    \"LW\": \"Left Wing\",\n",
    "\n",
    "    # forwards\n",
    "    \"CF\": \"Center Forward\",\n",
    "    \"ST\": \"Center Forward\",\n",
    "}\n",
    "\n",
    "CANONICAL_POSITIONS = [\n",
    "    \"Goalkeeper\",\n",
    "    \"Right Back\", \"Center Back\", \"Left Back\",\n",
    "    \"Defensive Midfield\",\n",
    "    \"Center Midfield\",\n",
    "    \"Attacking Midfield\",\n",
    "    \"Right Wing\", \"Left Wing\",\n",
    "    \"Center Forward\",\n",
    "]\n",
    "\n",
    "def normalize_grouped_position(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Strip whitespace and map variants to your canonical 10 names.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    return s.replace(POSITION_NORMALIZER)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ------------- helpers -------------\n",
    "def _detect_per90_rw_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return sorted([c for c in df.columns if c.endswith(\"_per90_rw\")])\n",
    "\n",
    "def _top_feature_label(centroid: np.ndarray, feature_names: List[str], k_top: int = 3) -> str:\n",
    "    idx = np.argsort(np.abs(centroid))[::-1][:k_top]\n",
    "    return \", \".join([(\"+\" if centroid[j] >= 0 else \"-\") + feature_names[j] for j in idx])\n",
    "\n",
    "def _nearest_centroid(vec: np.ndarray, cents: np.ndarray) -> int:\n",
    "    d = norm(cents - vec[None, :], axis=1)\n",
    "    return int(np.argmin(d))\n",
    "\n",
    "def _merge_small_clusters(\n",
    "    X: np.ndarray, labels: np.ndarray, cents: np.ndarray, min_cluster_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Any cluster with size < min_cluster_size gets reassigned to the nearest\n",
    "    centroid among clusters that already satisfy the minimum (or the largest).\n",
    "    Recompute until stable or one pass is enough for practical usage.\n",
    "    \"\"\"\n",
    "    lbl = labels.copy()\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        # sizes by current labels\n",
    "        unique, counts = np.unique(lbl, return_counts=True)\n",
    "        size_map = dict(zip(unique, counts))\n",
    "        ok_clusters = [c for c, sz in size_map.items() if sz >= min_cluster_size]\n",
    "        if not ok_clusters:\n",
    "            # fallback: keep a single biggest cluster as ok\n",
    "            biggest = int(unique[np.argmax(counts)])\n",
    "            ok_clusters = [biggest]\n",
    "\n",
    "        for c in unique:\n",
    "            if size_map[c] < min_cluster_size:\n",
    "                # reassign all points of cluster c\n",
    "                mask = (lbl == c)\n",
    "                # nearest centroid among ok_clusters\n",
    "                ok_centroids = cents[ok_clusters]\n",
    "                # recompute nearest for each point\n",
    "                for i in np.where(mask)[0]:\n",
    "                    new_c_rel = _nearest_centroid(X[i], ok_centroids)\n",
    "                    new_c = ok_clusters[new_c_rel]\n",
    "                    if new_c != lbl[i]:\n",
    "                        lbl[i] = new_c\n",
    "                        changed = True\n",
    "\n",
    "        # update sizes & stop if no change\n",
    "    return lbl\n",
    "\n",
    "def _recompute_centroids(Z: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    cents = []\n",
    "    for k in np.unique(labels):\n",
    "        cents.append(Z[labels == k].mean(axis=0))\n",
    "    return np.vstack(cents)\n",
    "\n",
    "def _kmeans_two_with_min_size(\n",
    "    Z: np.ndarray,\n",
    "    min_size: int,\n",
    "    random_state: int = 7,\n",
    "    n_init: int = 20,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Force K=2. If one cluster < min_size and n >= 2*min_size, move the closest\n",
    "    points from the larger cluster to the smaller until the smaller reaches min_size.\n",
    "    Returns (labels, centroids) in 2-cluster indexing {0,1}.\n",
    "    \"\"\"\n",
    "    n = Z.shape[0]\n",
    "    km = KMeans(n_clusters=2, random_state=random_state, n_init=n_init)\n",
    "    labels = km.fit_predict(Z)\n",
    "    cents = km.cluster_centers_\n",
    "\n",
    "    # Current sizes\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "    size_map = dict(zip(uniq, counts))\n",
    "    if min(size_map.values()) >= min_size:\n",
    "        return labels, cents\n",
    "\n",
    "    if n < 2 * min_size:\n",
    "        print(f\"[GK] Warning: n={n} < 2*min_size={2*min_size}. \"\n",
    "              f\"Keeping K=2 split (cannot satisfy min_size for both).\")\n",
    "        return labels, cents\n",
    "\n",
    "    # Identify small/big clusters\n",
    "    small = min(size_map, key=size_map.get)\n",
    "    big   = 1 - small\n",
    "    need  = min_size - size_map[small]\n",
    "    if need <= 0:\n",
    "        return labels, cents\n",
    "\n",
    "    # Distances of all points in the big cluster to the small centroid\n",
    "    big_idx = np.where(labels == big)[0]\n",
    "    d = norm(Z[big_idx] - cents[small][None, :], axis=1)\n",
    "    # Sort candidates by closeness to small centroid\n",
    "    order = np.argsort(d)\n",
    "\n",
    "    moved = 0\n",
    "    for idx in big_idx[order]:\n",
    "        # Move this point to small\n",
    "        labels[idx] = small\n",
    "        moved += 1\n",
    "        # Check sizes after each move\n",
    "        uniq, counts = np.unique(labels, return_counts=True)\n",
    "        size_map = dict(zip(uniq, counts))\n",
    "        if size_map[small] >= min_size:\n",
    "            break\n",
    "        # If big would drop below min_size by continuing, stop (avoid creating a new violation)\n",
    "        if size_map[big] - 1 < min_size and moved < need:\n",
    "            break\n",
    "\n",
    "    # Final centroids after reassignment\n",
    "    cents = _recompute_centroids(Z, labels)\n",
    "    # Sanity: still 2 clusters\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        print(\"[GK] Warning: reassignment collapsed to 1 cluster; reverting to initial split.\")\n",
    "        labels = km.labels_\n",
    "        cents  = km.cluster_centers_\n",
    "\n",
    "    return labels.astype(int), cents\n",
    "\n",
    "\n",
    "# ------------- main clustering -------------\n",
    "def cluster_players_per_grouped_position_team_agnostic_balanced(\n",
    "    rw_profiles: pd.DataFrame,\n",
    "    prim_pos: pd.DataFrame,    # keys + [\"primary_pos\",\"primary_games\"] (grouped names)\n",
    "    keys: List[str] = [\"player_id\",\"player\"],\n",
    "    n_clusters_by_pos: Optional[Dict[str, int]] = None,\n",
    "    default_k: Optional[Dict[str, int]] = None,\n",
    "    random_state: int = 7,\n",
    "    min_group_size_for_kmeans: int = 8,\n",
    "    min_cluster_size: int = 6,\n",
    "    min_games_in_pos: int = 5,\n",
    "    use_pca: bool = True,\n",
    "    pca_components: int = 8,\n",
    "    gk_force_two: bool = True,\n",
    "    gk_min_cluster_size: int = 15,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Static-K per grouped position using *_per90_rw features, but:\n",
    "      - filters players with < min_games_in_pos for that grouped pos,\n",
    "      - downshifts K until all clusters meet min_cluster_size (or K=1),\n",
    "      - merges any small clusters into nearest large centroid (safety),\n",
    "      - standardizes features; optional PCA to stabilize shapes.\n",
    "\n",
    "    Returns a DataFrame with: keys + [primary_grouped_pos, games_in_pos, pos_cluster_id, pos_cluster_label]\n",
    "    \"\"\"\n",
    "    # default K (if not provided)\n",
    "    _default_k = {\n",
    "        \"Goalkeeper\": 3,\n",
    "        \"Right Back\": 4, \"Left Back\": 4,\n",
    "        \"Center Back\": 4,\n",
    "        \"Defensive Midfield\": 4,\n",
    "        \"Center Midfield\": 4,\n",
    "        \"Attacking Midfield\": 4,\n",
    "        \"Right Wing\": 4, \"Left Wing\": 4,\n",
    "        \"Center Forward\": 5,\n",
    "    }\n",
    "    if n_clusters_by_pos is None and default_k is None:\n",
    "        n_clusters_by_pos = _default_k\n",
    "    elif n_clusters_by_pos is None:\n",
    "        n_clusters_by_pos = default_k\n",
    "\n",
    "    per90_cols = _detect_per90_rw_cols(rw_profiles)\n",
    "    if not per90_cols:\n",
    "        raise ValueError(\"No *_per90_rw in rw_profiles.\")\n",
    "\n",
    "    # Join primary grouped position + games\n",
    "    df = rw_profiles.merge(\n",
    "        prim_pos.rename(columns={\"primary_pos\": \"primary_grouped_pos\",\n",
    "                                 \"primary_games\": \"games_in_pos\"}),\n",
    "        on=keys, how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Filter by min games in that grouped position\n",
    "    df = df[df[\"games_in_pos\"].fillna(0).astype(int) >= int(min_games_in_pos)].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No players meet min_games_in_pos for clustering.\")\n",
    "\n",
    "    results = []\n",
    "    for pos, g in df.groupby(\"primary_grouped_pos\", dropna=False):\n",
    "        g = g.copy()\n",
    "        X_raw = g[per90_cols].astype(float).fillna(0.0).values\n",
    "\n",
    "        if len(g) < max(min_group_size_for_kmeans, 2) or np.nanmax(np.var(X_raw, axis=0)) < 1e-9:\n",
    "            labels = np.zeros(len(g), dtype=int)\n",
    "            cents = None\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            Z = scaler.fit_transform(X_raw)\n",
    "\n",
    "            if use_pca:\n",
    "                n_comp = min(pca_components, Z.shape[1], max(2, Z.shape[0]-1))\n",
    "                pca = PCA(n_components=n_comp, random_state=random_state)\n",
    "                Z = pca.fit_transform(Z)\n",
    "\n",
    "            if gk_force_two and pos == \"Goalkeeper\":\n",
    "                # ---- GK: FORCE K=2 with per-cluster min size ----\n",
    "                labels, cents = _kmeans_two_with_min_size(\n",
    "                    Z, min_size=gk_min_cluster_size, random_state=random_state, n_init=20\n",
    "                )\n",
    "            else:\n",
    "                # ---- non-GK: your balanced static-K with downshift + safety merge ----\n",
    "                K = int(n_clusters_by_pos.get(pos, 3))\n",
    "                K = max(1, min(K, len(g)))\n",
    "                km = None\n",
    "                labels = None\n",
    "                while K >= 1:\n",
    "                    km = KMeans(n_clusters=K, random_state=random_state, n_init=20)\n",
    "                    labels = km.fit_predict(Z)\n",
    "                    uniq, counts = np.unique(labels, return_counts=True)\n",
    "                    if (counts >= min_cluster_size).all() or K == 1:\n",
    "                        break\n",
    "                    K -= 1\n",
    "                cents = km.cluster_centers_ if km is not None else None\n",
    "\n",
    "                # safety merge small clusters (if any) into nearest big centroid\n",
    "                if cents is not None:\n",
    "                    uniq, counts = np.unique(labels, return_counts=True)\n",
    "                    if (counts < min_cluster_size).any() and len(uniq) > 1:\n",
    "                        labels = _merge_small_clusters(Z, labels, cents, min_cluster_size)\n",
    "                        # recompute centroids after merge for labeling\n",
    "                        cents = _recompute_centroids(Z, labels)\n",
    "\n",
    "        labels = labels.astype(int)\n",
    "        # --- labeling (top features) ---\n",
    "        if cents is None:\n",
    "            label_texts = [\"small-group\"] * len(g)\n",
    "        else:\n",
    "            def top_text_for_label(kidx: int) -> str:\n",
    "                centroid_std = cents[kidx]\n",
    "                if use_pca:\n",
    "                    centroid_std_full = (centroid_std @ pca.components_)\n",
    "                else:\n",
    "                    centroid_std_full = centroid_std\n",
    "                return _top_feature_label(centroid_std_full, per90_cols, k_top=3)\n",
    "            label_texts = [top_text_for_label(l) for l in labels]\n",
    "\n",
    "        sub = g[keys + [\"primary_grouped_pos\",\"games_in_pos\"]].copy()\n",
    "        sub[\"pos_cluster_id\"] = labels\n",
    "        sub[\"pos_cluster_label\"] = [f\"{pos} | {t}\" for t in label_texts]\n",
    "        results.append(sub)\n",
    "\n",
    "    return pd.concat(results, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acb5237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# E) Targets/appearances and design matrix; twin models: xG For & xG Against\n",
    "# ------------------------------------------------------------------------------\n",
    "def build_match_targets_and_appearances(\n",
    "    events: pd.DataFrame,\n",
    "    xg_col: str = \"shot_statsbomb_xg\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    req_cols = {\"match_id\", \"team_id\", \"team\", \"type\", \"player_id\"}\n",
    "    missing = req_cols - set(events.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"events missing: {missing}\")\n",
    "    ev = events.copy()\n",
    "    if xg_col not in ev.columns:\n",
    "        ev[xg_col] = 0.0\n",
    "    ev[xg_col] = pd.to_numeric(ev[xg_col], errors=\"coerce\").fillna(0.0)\n",
    "    shots = ev.loc[ev[\"type\"].eq(\"Shot\"), [\"match_id\",\"team_id\",\"team\", xg_col]].copy()\n",
    "    xg_for = (shots.groupby([\"match_id\",\"team_id\",\"team\"], as_index=False)[xg_col]\n",
    "                   .sum().rename(columns={xg_col: \"xg_for\"}))\n",
    "    xg_match = xg_for.groupby(\"match_id\", as_index=False)[\"xg_for\"].sum().rename(columns={\"xg_for\":\"xg_total_match\"})\n",
    "    targets = xg_for.merge(xg_match, on=\"match_id\", how=\"left\")\n",
    "    targets[\"xg_against\"] = targets[\"xg_total_match\"] - targets[\"xg_for\"]\n",
    "    targets = targets.drop(columns=[\"xg_total_match\"])\n",
    "    # controls\n",
    "    if {\"home_team\",\"away_team\"}.issubset(ev.columns):\n",
    "        ha = ev.groupby(\"match_id\")[[\"home_team\",\"away_team\"]].first().reset_index()\n",
    "        targets = targets.merge(ha, on=\"match_id\", how=\"left\")\n",
    "        targets[\"is_home\"] = (targets[\"team\"] == targets[\"home_team\"]).astype(int)\n",
    "        targets = targets.drop(columns=[\"home_team\",\"away_team\"])\n",
    "    else:\n",
    "        targets[\"is_home\"] = 0\n",
    "    # appearances\n",
    "    appearances = (ev.dropna(subset=[\"player_id\"])\n",
    "                    .groupby([\"match_id\",\"team_id\"])[\"player_id\"]\n",
    "                    .unique().reset_index()\n",
    "                    .rename(columns={\"player_id\":\"player_ids\"}))\n",
    "    return targets, appearances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0d9da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DesignMatrixBuilder:\n",
    "    min_app_per_player: int = 5\n",
    "    role_col: str = \"pos_cluster_id\"\n",
    "    def build(self, targets: pd.DataFrame, appearances: pd.DataFrame,\n",
    "              player_roles: pd.DataFrame, id_cols_roles: List[str] = [\"player_id\"]\n",
    "              ) -> Tuple[pd.DataFrame, Dict[str, List[int]], List[str]]:\n",
    "        df = targets.merge(appearances, on=[\"match_id\",\"team_id\"], how=\"left\")\n",
    "        df[\"player_ids\"] = df[\"player_ids\"].apply(lambda x: x if isinstance(x, (list, np.ndarray)) else [])\n",
    "        pr = player_roles[id_cols_roles + [self.role_col]].drop_duplicates(\"player_id\").set_index(\"player_id\")\n",
    "        counts = df[\"player_ids\"].explode().value_counts()\n",
    "        kept_players = counts[counts >= self.min_app_per_player].index.astype(int).tolist()\n",
    "        player_list = sorted(kept_players); player_index = {pid:i for i,pid in enumerate(player_list)}\n",
    "        role_list = sorted(pr[self.role_col].dropna().astype(int).unique().tolist())\n",
    "        role_index = {rid:i for i,rid in enumerate(role_list)}\n",
    "\n",
    "        rows = []\n",
    "        for _, r in df.iterrows():\n",
    "            pids = []\n",
    "            for p in r[\"player_ids\"]:\n",
    "                try: ip = int(p); pids.append(ip)\n",
    "                except: continue\n",
    "            pids = [pid for pid in pids if pid in player_index]\n",
    "            row_p = np.zeros(len(player_list), dtype=float)\n",
    "            for pid in pids: row_p[player_index[pid]] = 1.0\n",
    "            row_r = np.zeros(len(role_list), dtype=float)\n",
    "            for pid in pids:\n",
    "                if pid in pr.index:\n",
    "                    rid = pr.loc[pid, self.role_col]\n",
    "                    if pd.notna(rid):\n",
    "                        rid=int(rid)\n",
    "                        if rid in role_index: row_r[role_index[rid]] += 1.0\n",
    "            rows.append({\"players\": row_p, \"roles\": row_r,\n",
    "                         \"is_home\": float(r.get(\"is_home\",0.0)),\n",
    "                         \"xg_for\": float(r.get(\"xg_for\",0.0)),\n",
    "                         \"xg_against\": float(r.get(\"xg_against\",0.0))})\n",
    "\n",
    "        # features\n",
    "        X_parts, colnames = [], []\n",
    "        if player_list:\n",
    "            P = np.vstack([rw[\"players\"] for rw in rows]) if rows else np.empty((0,len(player_list)))\n",
    "            P_cols = [f\"P_{pid}\" for pid in player_list]\n",
    "            X_parts.append(pd.DataFrame(P, columns=P_cols)); colnames += P_cols\n",
    "        if role_list:\n",
    "            R = np.vstack([rw[\"roles\"] for rw in rows]) if rows else np.empty((0,len(role_list)))\n",
    "            R_cols = [f\"R_{rid}\" for rid in role_list]\n",
    "            X_parts.append(pd.DataFrame(R, columns=R_cols)); colnames += R_cols\n",
    "        C = pd.DataFrame({\"is_home\":[rw[\"is_home\"] for rw in rows]})\n",
    "        X_parts.append(C); colnames.append(\"is_home\")\n",
    "        X = pd.concat(X_parts, axis=1).astype(float) if X_parts else pd.DataFrame()\n",
    "        y_for = pd.Series([rw[\"xg_for\"] for rw in rows], name=\"y_for\").astype(float)\n",
    "        y_against = pd.Series([rw[\"xg_against\"] for rw in rows], name=\"y_against\").astype(float)\n",
    "        mapping = {\"players\": player_list, \"roles\": role_list}\n",
    "        return X, mapping, colnames, y_for, y_against\n",
    "\n",
    "def train_two_ridge_models_for_against(X: pd.DataFrame, y_for: pd.Series, y_against: pd.Series) -> Tuple[Pipeline, Pipeline]:\n",
    "    def fit_one(y):\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"ridge\", RidgeCV(alphas=(0.1,0.3,1.0,3.0,10.0), cv=5, fit_intercept=True))\n",
    "        ])\n",
    "        pipe.fit(X.values, y.values)\n",
    "        return pipe\n",
    "    return fit_one(y_for), fit_one(y_against)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "275a1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# F) Latest XI for ANY team (team-agnostic helper)\n",
    "# ------------------------------------------------------------------------------\n",
    "def _parse_datetime_safe(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def get_last_starting_xi_ids(events_min: pd.DataFrame, team_name: str) -> Tuple[List[int], int]:\n",
    "    df = events_min.copy()\n",
    "    mt = df.loc[df[\"team\"] == team_name, [\"match_id\",\"match_date\"]].drop_duplicates(\"match_id\")\n",
    "    if mt.empty:\n",
    "        raise ValueError(f\"No matches found for team='{team_name}'.\")\n",
    "    if mt[\"match_date\"].notna().any():\n",
    "        mt[\"match_date\"] = _parse_datetime_safe(mt[\"match_date\"])\n",
    "        latest = mt.sort_values(\"match_date\").iloc[-1]\n",
    "    else:\n",
    "        latest = mt.sort_values(\"match_id\").iloc[-1]\n",
    "    match_id = int(latest[\"match_id\"])\n",
    "\n",
    "    emt = df[(df[\"match_id\"] == match_id) & (df[\"team\"] == team_name)].copy()\n",
    "    xi_rows = emt.loc[emt[\"type\"] == \"Starting XI\"]\n",
    "    if not xi_rows.empty and \"tactics\" in xi_rows.columns:\n",
    "        tac = xi_rows.iloc[0].get(\"tactics\", {})\n",
    "        lineup = tac.get(\"lineup\", []) if isinstance(tac, dict) else []\n",
    "        ids = [p.get(\"player_id\") for p in lineup if isinstance(p, dict) and p.get(\"player_id\") is not None]\n",
    "        if len(ids) >= 10:\n",
    "            return [int(x) for x in ids[:11]], match_id\n",
    "    # fallback: earliest 11 appearances\n",
    "    emt[\"tmin\"] = emt[\"minute\"].fillna(0).astype(float) + emt[\"second\"].fillna(0).astype(float)/60.0\n",
    "    first_seen = (emt.dropna(subset=[\"player_id\"])\n",
    "                    .groupby([\"player_id\",\"player\"], as_index=False)[\"tmin\"]\n",
    "                    .min().sort_values(\"tmin\").head(11))\n",
    "    return first_seen[\"player_id\"].astype(int).tolist(), match_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec1f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _detect_per90_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return sorted([c for c in df.columns if c.endswith(\"_per90\")])\n",
    "\n",
    "def _detect_per90_rw_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return sorted([c for c in df.columns if c.endswith(\"_per90_rw\")])\n",
    "\n",
    "def _season_key(s):\n",
    "    try:\n",
    "        return int(str(s).split(\"-\")[0])\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "# ---------- A) Team-season style (aggregate) ----------\n",
    "def build_team_season_style(events_all: pd.DataFrame,\n",
    "                            stat_cols_team: Optional[List[str]] = None,\n",
    "                            min_minutes: float = 0.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate team-season style features from events. For v1 we reuse per90-like\n",
    "    columns you already carry at event level (xg, xA, passes, obv, etc.).\n",
    "    If you only have player-level season tables, you can aggregate from that instead.\n",
    "    \"\"\"\n",
    "    df = events_all.copy()\n",
    "\n",
    "    # If you already have a match-level team table, use it here instead of raw events.\n",
    "    # Fallback: compute a small subset from events (count-based) – adjust to your schema.\n",
    "    # Here we assume you have 'season', 'team', and columns like obv_* or pass_* at event rows.\n",
    "\n",
    "    # Example: team-season counts (very light)\n",
    "    group = [\"season\", \"team\"]\n",
    "    # Define a few robust team-level proxies (tweak/expand to your columns)\n",
    "    agg_map = {\n",
    "        \"pass_pass_success_probability\": \"mean\",\n",
    "        \"pass_length\": \"mean\",\n",
    "        \"shot_statsbomb_xg\": \"sum\",\n",
    "        \"duel_outcome\": \"count\",            # proxy for intensity\n",
    "        \"counterpress\": \"sum\",\n",
    "        \"obv_total_net\": \"sum\",\n",
    "    }\n",
    "    has = [c for c in agg_map if c in df.columns]\n",
    "    if not has:\n",
    "        raise ValueError(\"build_team_season_style: no suitable columns in events_all; adjust agg_map.\")\n",
    "    team_season = (df.groupby(group)[has].agg(agg_map).reset_index())\n",
    "\n",
    "    # normalize sizes to per-game if you have games per season per team\n",
    "    if \"match_id\" in df.columns:\n",
    "        gpg = (df.drop_duplicates([\"season\", \"team\", \"match_id\"])\n",
    "                 .groupby([\"season\",\"team\"]).size().reset_index(name=\"n_games\"))\n",
    "        team_season = team_season.merge(gpg, on=[\"season\",\"team\"], how=\"left\")\n",
    "        for col in [\"shot_statsbomb_xg\", \"duel_outcome\", \"counterpress\", \"obv_total_net\"]:\n",
    "            if col in team_season.columns:\n",
    "                team_season[col] = team_season[col] / team_season[\"n_games\"].replace(0, np.nan)\n",
    "\n",
    "    # PCA to style components (keep 3 by default)\n",
    "    feat_cols = [c for c in team_season.columns if c not in [\"season\",\"team\",\"n_games\"]]\n",
    "    X = team_season[feat_cols].astype(float).fillna(0.0).values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xn = scaler.transform(X)\n",
    "    pca = PCA(n_components=min(3, Xn.shape[1])).fit(Xn)\n",
    "    Z = pca.transform(Xn)\n",
    "    for i in range(Z.shape[1]):\n",
    "        team_season[f\"style_pc{i+1}\"] = Z[:, i]\n",
    "\n",
    "    meta = {\"scaler_means\": scaler.mean_.tolist(),\n",
    "            \"scaler_scales\": scaler.scale_.tolist(),\n",
    "            \"pca_components\": pca.components_.tolist(),\n",
    "            \"pca_mean\": pca.mean_.tolist(),\n",
    "            \"feat_cols\": feat_cols}\n",
    "\n",
    "    # keep only components and keys\n",
    "    keep = [\"season\",\"team\"] + [f\"style_pc{i+1}\" for i in range(Z.shape[1])]\n",
    "    return team_season[keep], meta\n",
    "\n",
    "# ---------- B) Team-season lineup role mix ----------\n",
    "def build_team_season_role_mix(events_all: pd.DataFrame,\n",
    "                               clusters: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each team-season, count appearances by pos_cluster_id in Starting XI (or first 11 seen).\n",
    "    Normalize to proportions.\n",
    "    \"\"\"\n",
    "    df = events_all.copy()\n",
    "    # find XI per match per team\n",
    "    xi = df[(df[\"type\"]==\"Starting XI\") & df[\"tactics\"].notna()][[\"season\",\"team\",\"match_id\",\"tactics\"]].copy()\n",
    "    rows = []\n",
    "    for _, r in xi.iterrows():\n",
    "        lineup = r[\"tactics\"].get(\"lineup\", []) if isinstance(r[\"tactics\"], dict) else []\n",
    "        pids = [p.get(\"player_id\") for p in lineup if isinstance(p, dict)]\n",
    "        for pid in pids:\n",
    "            rows.append({\"season\": r[\"season\"], \"team\": r[\"team\"], \"match_id\": r[\"match_id\"], \"player_id\": pid})\n",
    "    xi_long = pd.DataFrame(rows)\n",
    "    if xi_long.empty:\n",
    "        # fallback: first 11 seen by minute\n",
    "        df[\"tmin\"] = df[\"minute\"].fillna(0) + df[\"second\"].fillna(0)/60.0\n",
    "        tmp = (df.dropna(subset=[\"player_id\"])\n",
    "                 .sort_values([\"season\",\"team\",\"match_id\",\"tmin\"])\n",
    "                 .groupby([\"season\",\"team\",\"match_id\"])\n",
    "                 .head(11)[[\"season\",\"team\",\"match_id\",\"player_id\"]])\n",
    "        xi_long = tmp\n",
    "\n",
    "    role = clusters.drop_duplicates(\"player_id\")[[\"player_id\",\"pos_cluster_id\"]]\n",
    "    xi_role = xi_long.merge(role, on=\"player_id\", how=\"left\").dropna(subset=[\"pos_cluster_id\"])\n",
    "    cnt = (xi_role.groupby([\"season\",\"team\",\"pos_cluster_id\"])\n",
    "                  .size().reset_index(name=\"count\"))\n",
    "    # convert to wide proportions\n",
    "    total = cnt.groupby([\"season\",\"team\"])[\"count\"].sum().reset_index(name=\"total\")\n",
    "    cnt = cnt.merge(total, on=[\"season\",\"team\"], how=\"left\")\n",
    "    cnt[\"prop\"] = cnt[\"count\"] / cnt[\"total\"].replace(0, np.nan)\n",
    "    wide = (cnt.pivot(index=[\"season\",\"team\"],\n",
    "                      columns=\"pos_cluster_id\",\n",
    "                      values=\"prop\")\n",
    "               .fillna(0.0).reset_index())\n",
    "    wide.columns = [\"season\",\"team\"] + [f\"role_mix_{int(c)}\" for c in wide.columns[2:]]\n",
    "    return wide\n",
    "\n",
    "# ---------- C) Assemble training table ----------\n",
    "def prepare_network_training(\n",
    "    seasonal_table: pd.DataFrame,           # player-team-season per90 targets\n",
    "    rw_profiles: pd.DataFrame,              # player baseline per90_rw\n",
    "    clusters: pd.DataFrame,                 # for pos + role ids\n",
    "    team_style: pd.DataFrame,               # style_pc1..k\n",
    "    team_role_mix: pd.DataFrame,            # role_mix_*\n",
    "    keys: List[str] = [\"player_id\",\"player\"]\n",
    ") -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Build training rows: y = season per90; X = [player RW baseline, team style PCs, role mix].\n",
    "    Returns (train_df, feature_cols, target_cols).\n",
    "    \"\"\"\n",
    "    # targets\n",
    "    per90_cols = _detect_per90_cols(seasonal_table)\n",
    "    base = seasonal_table[keys + [\"team\",\"season\"] + per90_cols].copy()\n",
    "\n",
    "    # baseline RW\n",
    "    rw = rw_profiles[keys + _detect_per90_rw_cols(rw_profiles)].copy()\n",
    "\n",
    "    # roles / positions\n",
    "    role_lookup = clusters.drop_duplicates(\"player_id\")[[\"player_id\",\"pos_cluster_id\",\"primary_grouped_pos\"]]\n",
    "\n",
    "    # join\n",
    "    df = (base\n",
    "          .merge(rw, on=keys, how=\"left\")\n",
    "          .merge(role_lookup, on=\"player_id\", how=\"left\")\n",
    "          .merge(team_style, on=[\"season\",\"team\"], how=\"left\")\n",
    "          .merge(team_role_mix, on=[\"season\",\"team\"], how=\"left\")\n",
    "          )\n",
    "\n",
    "    # feature columns\n",
    "    X_cols = _detect_per90_rw_cols(df)  # player RW baseline features\n",
    "    X_cols += [c for c in df.columns if c.startswith(\"style_pc\")]\n",
    "    X_cols += [c for c in df.columns if c.startswith(\"role_mix_\")]\n",
    "    # Optional: add position as one-hot\n",
    "    if \"primary_grouped_pos\" in df.columns:\n",
    "        pos_dum = pd.get_dummies(df[\"primary_grouped_pos\"], prefix=\"pos\")\n",
    "        df = pd.concat([df, pos_dum], axis=1)\n",
    "        X_cols += list(pos_dum.columns)\n",
    "\n",
    "    # targets: all per90 in seasonal table\n",
    "    y_cols = per90_cols\n",
    "\n",
    "    # Clean NA\n",
    "    df = df.dropna(subset=[\"team\",\"season\"])\n",
    "    return df, X_cols, y_cols\n",
    "\n",
    "# ---------- D) Train one model per stat ----------\n",
    "def train_network_effects_models(train_df: pd.DataFrame,\n",
    "                                 feature_cols: List[str],\n",
    "                                 target_cols: List[str]) -> Dict[str, Pipeline]:\n",
    "    \"\"\"\n",
    "    Trains RidgeCV models per target stat. Returns dict stat -> sklearn Pipeline(StandardScaler + RidgeCV).\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for y in target_cols:\n",
    "        X = train_df[feature_cols].astype(float).fillna(0.0).values\n",
    "        yv = train_df[y].astype(float).fillna(0.0).values\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"ridge\",  RidgeCV(alphas=np.logspace(-3, 3, 13), cv=5))\n",
    "        ])\n",
    "        pipe.fit(X, yv)\n",
    "        models[y] = pipe\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c00834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- E) Scoring utilities ----------\n",
    "def _player_rw_vector(rw_profiles: pd.DataFrame, player_id: int) -> pd.Series:\n",
    "    row = rw_profiles[rw_profiles[\"player_id\"] == player_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"RW profile not found for player_id={player_id}\")\n",
    "    return row.iloc[0]\n",
    "\n",
    "def _player_position_onehot(pos: str, all_pos_names: List[str]) -> pd.Series:\n",
    "    o = {f\"pos_{p}\": 0.0 for p in all_pos_names}\n",
    "    if pos in all_pos_names:\n",
    "        o[f\"pos_{pos}\"] = 1.0\n",
    "    return pd.Series(o, dtype=float)\n",
    "\n",
    "def build_feature_row_for_context(\n",
    "    player_id: int,\n",
    "    rw_profiles: pd.DataFrame,\n",
    "    clusters: pd.DataFrame,\n",
    "    team_style: pd.DataFrame,\n",
    "    team_role_mix: pd.DataFrame,\n",
    "    season: str,\n",
    "    team: str,\n",
    "    feature_cols: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compose a single-row DataFrame of features for (player, season, team) context.\n",
    "    \"\"\"\n",
    "    rw_row = _player_rw_vector(rw_profiles, player_id)\n",
    "    pos = clusters.loc[clusters[\"player_id\"] == player_id, \"primary_grouped_pos\"]\n",
    "    pos = pos.iloc[0] if not pos.empty else \"Center Midfield\"\n",
    "\n",
    "    # style pcs\n",
    "    sty = team_style[(team_style[\"season\"]==season)&(team_style[\"team\"]==team)]\n",
    "    if sty.empty:\n",
    "        raise ValueError(f\"No team_style for team={team} season={season}\")\n",
    "    # role mix\n",
    "    mix = team_role_mix[(team_role_mix[\"season\"]==season)&(team_role_mix[\"team\"]==team)]\n",
    "    if mix.empty:\n",
    "        # zeros if not found\n",
    "        mix = pd.DataFrame([{\"season\":season,\"team\":team}])\n",
    "\n",
    "    # assemble\n",
    "    pos_dummies = [c for c in feature_cols if c.startswith(\"pos_\")]\n",
    "    uniq_pos = [c.replace(\"pos_\",\"\") for c in pos_dummies]\n",
    "    pos_oh = _player_position_onehot(pos, uniq_pos)\n",
    "\n",
    "    feat = pd.Series(dtype=float)\n",
    "    for c in feature_cols:\n",
    "        if c in rw_row.index:\n",
    "            feat[c] = float(rw_row[c])\n",
    "        elif c in sty.columns:\n",
    "            feat[c] = float(sty.iloc[0][c])\n",
    "        elif c in mix.columns:\n",
    "            feat[c] = float(mix.iloc[0][c])\n",
    "        elif c in pos_oh.index:\n",
    "            feat[c] = float(pos_oh[c])\n",
    "        else:\n",
    "            feat[c] = 0.0\n",
    "    return pd.DataFrame([feat.values], columns=feature_cols)\n",
    "\n",
    "def predict_player_in_context(models: Dict[str, Pipeline],\n",
    "                              feature_row: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Apply trained per-stat models to one feature row. Returns stat -> predicted per90.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for stat, mdl in models.items():\n",
    "        out[stat] = float(mdl.predict(feature_row.values)[0])\n",
    "    return out\n",
    "\n",
    "# ---------- F) Adjust role mix for a lineup swap (impact on others) ----------\n",
    "def adjust_role_mix_for_swap(team_mix_row: pd.Series,\n",
    "                             in_role_id: int,\n",
    "                             out_role_id: Optional[int] = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Adjust a team role_mix_* vector when one incoming role replaces an outgoing role.\n",
    "    Renormalizes to sum=1 over role_mix_*.\n",
    "    \"\"\"\n",
    "    ser = team_mix_row.copy()\n",
    "    cols = [c for c in ser.index if c.startswith(\"role_mix_\")]\n",
    "    if not cols:\n",
    "        return ser\n",
    "\n",
    "    def bump(role_id, delta):\n",
    "        col = f\"role_mix_{int(role_id)}\"\n",
    "        if col in ser.index:\n",
    "            ser[col] = max(0.0, float(ser[col]) + delta)\n",
    "\n",
    "    bump(in_role_id, +1.0)\n",
    "    if out_role_id is not None:\n",
    "        bump(out_role_id, -1.0)\n",
    "\n",
    "    tot = float(ser[cols].sum())\n",
    "    if tot > 0:\n",
    "        ser[cols] = ser[cols] / tot\n",
    "    return ser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28cdd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.cluster_namer import name_cluster_from_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f388f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporter.py (replace your current export_artifacts with this)\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle\n",
    "from typing import Optional, Dict\n",
    "from datetime import date\n",
    "\n",
    "# import the balanced clusterer you added:\n",
    "# from your_module import cluster_players_per_grouped_position_team_agnostic_balanced\n",
    "\n",
    "# If you want this file self-contained, import like:\n",
    "# from .clustering import cluster_players_per_grouped_position_team_agnostic_balanced\n",
    "\n",
    "# -------------------------\n",
    "# Static K per grouped position (your preferred defaults)\n",
    "# -------------------------\n",
    "DEFAULT_K_STATIC: Dict[str, int] = {\n",
    "    \"Goalkeeper\": 3,\n",
    "    \"Right Back\": 4, \"Left Back\": 4,\n",
    "    \"Center Back\": 4,\n",
    "    \"Defensive Midfield\": 4,\n",
    "    \"Center Midfield\": 4,\n",
    "    \"Attacking Midfield\": 4,\n",
    "    \"Right Wing\": 4, \"Left Wing\": 4,\n",
    "    \"Center Forward\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "def export_artifacts(\n",
    "    events_all: pd.DataFrame,\n",
    "    seasonal_table: pd.DataFrame,\n",
    "    output_dir: str | Path,\n",
    "    *,\n",
    "    # ---- positions / clusters controls ----\n",
    "    min_games_pos: int = 1,                 # to compute primary/secondary/third in top_positions\n",
    "    min_games_in_pos_for_cluster: int = 5,   # filter before clustering (per grouped pos)\n",
    "    min_group_size_for_kmeans: int = 8,      # if fewer rows, assign 1 cluster\n",
    "    min_cluster_size: int = 8,               # downshift K until all clusters >= this\n",
    "    use_pca: bool = True,\n",
    "    pca_components: int = 8,\n",
    "    # override K per pos if you want a different static dictionary:\n",
    "    n_clusters_by_pos_override: Optional[Dict[str, int]] = None,\n",
    "    # ---- RAPM / lineup model controls ----\n",
    "    min_app_per_player: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds & saves:\n",
    "      - rw_per90_team_agnostic.parquet\n",
    "      - players_meta.parquet\n",
    "      - clusters_team_agnostic.parquet (w/ pos_cluster_name)\n",
    "      - role_lookup.parquet\n",
    "      - events_min.parquet\n",
    "      - rapm_for.pkl / rapm_against.pkl\n",
    "      - feature_names.json / mapping.json\n",
    "\n",
    "    Notes:\n",
    "      * Clustering uses balanced static-K with:\n",
    "        - min games in grouped position,\n",
    "        - downshift K until all clusters >= min_cluster_size,\n",
    "        - safety merge for tiny clusters,\n",
    "        - z-score (and optional PCA).\n",
    "    \"\"\"\n",
    "    outdir = Path(output_dir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Team-agnostic RW per-90 (recency weighted)\n",
    "    rw = build_recency_weighted_per90_team_agnostic(seasonal_table, keys=[\"player_id\", \"player\"], K=4)\n",
    "    rw.to_parquet(outdir / \"rw_per90_team_agnostic.parquet\", index=False)\n",
    "\n",
    "    # 2) Latest team label (one row per player_id)\n",
    "    latest_team = latest_team_per_player(events_all)\n",
    "\n",
    "    # 3) Top positions (>= min_games_pos) → grouped names + games\n",
    "    #    Returns primary/secondary/third (grouped) + primary_games\n",
    "    wide_pos, _ = top_positions_grouped_team_agnostic(\n",
    "        events_all, keys=[\"player_id\", \"player\"], min_games=min_games_pos\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Clusters (balanced static-K)\n",
    "    # -----------------------------\n",
    "    prim = (wide_pos[[\"player_id\", \"player\", \"primary_pos\", \"primary_games\"]]\n",
    "            .dropna(subset=[\"primary_pos\"]))\n",
    "\n",
    "    # choose dict of K per position\n",
    "    k_per_pos = n_clusters_by_pos_override or DEFAULT_K_STATIC\n",
    "\n",
    "    clusters = cluster_players_per_grouped_position_team_agnostic_balanced(\n",
    "        rw_profiles=rw,\n",
    "        prim_pos=prim,                         # expects grouped 'primary_pos' + 'primary_games'\n",
    "        keys=[\"player_id\", \"player\"],\n",
    "        n_clusters_by_pos=k_per_pos,\n",
    "        # balancing knobs\n",
    "        min_games_in_pos=min_games_in_pos_for_cluster,\n",
    "        min_group_size_for_kmeans=min_group_size_for_kmeans,\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        use_pca=use_pca,\n",
    "        pca_components=pca_components,\n",
    "    )\n",
    "\n",
    "    # Human-readable archetype name (uses your namer from modules/)\n",
    "    clusters[\"pos_cluster_name\"] = clusters.apply(\n",
    "        lambda r: name_cluster_from_label(\n",
    "            pos=str(r[\"primary_grouped_pos\"]),\n",
    "            label=str(r[\"pos_cluster_label\"])\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    clusters.to_parquet(outdir / \"clusters_team_agnostic.parquet\", index=False)\n",
    "\n",
    "    # 5) role_lookup (player_id -> pos_cluster_id) for the lineup model\n",
    "    role_lookup = clusters.drop_duplicates(\"player_id\")[[\"player_id\", \"pos_cluster_id\"]]\n",
    "    role_lookup.to_parquet(outdir / \"role_lookup.parquet\", index=False)\n",
    "\n",
    "    # 6) players_meta (latest_team + top3 positions)\n",
    "    players_meta = latest_team.merge(\n",
    "        wide_pos[[\"player_id\", \"player\", \"primary_pos\", \"secondary_pos\", \"third_pos\"]],\n",
    "        on=[\"player_id\", \"player\"], how=\"left\"\n",
    "    )\n",
    "    keep_cols = [\"player_id\", \"birth_date\", \"country\"]\n",
    "    for c in keep_cols:\n",
    "        if c not in df_players.columns:\n",
    "            df_players[c] = pd.NA\n",
    "    u = df_players[keep_cols].drop_duplicates(\"player_id\").copy()\n",
    "\n",
    "    # 4) Compute age from birth_date (today as reference)\n",
    "    def compute_age_series(birth_series: pd.Series, ref: date = date.today()) -> pd.Series:\n",
    "        bd = pd.to_datetime(birth_series, errors=\"coerce\", utc=True).dt.date\n",
    "        # precise age in years: floor((ref - bd).days / 365.2425)\n",
    "        days = (pd.Series(ref, index=bd.index) - bd).dt.days\n",
    "        return np.floor(days / 365.2425).astype(\"Int64\")\n",
    "\n",
    "    u[\"age\"] = compute_age_series(u[\"birth_date\"])\n",
    "\n",
    "    # 5) Merge into players_meta by player_id (left merge, only adds columns)\n",
    "    enriched = (players_meta\n",
    "                .merge(u, on=\"player_id\", how=\"left\", suffixes=(\"\", \"_from_lineups\")))\n",
    "\n",
    "    # 6) (Optional) if players_meta already had 'country' or 'birth_date', prefer non-null existing\n",
    "    for col in [\"country\", \"birth_date\"]:\n",
    "        alt = f\"{col}_from_lineups\"\n",
    "        if alt in enriched.columns:\n",
    "            # keep existing non-null; fill gaps from lineup data\n",
    "            enriched[col] = enriched[col].where(enriched[col].notna(), enriched[alt])\n",
    "            enriched.drop(columns=[alt], inplace=True)\n",
    "\n",
    "    # 7) Save back to the SAME artifact path so the app’s existing load keeps working\n",
    "    #    (We only added columns: 'birth_date' (if missing), 'country' (filled), and new 'age')\n",
    "\n",
    "    enriched.to_parquet(outdir / \"players_meta.parquet\", index=False)\n",
    "\n",
    "    # 7) events_min (slim subset for latest XI extraction in the app)\n",
    "    events_min = build_events_min(events_all)\n",
    "    events_min.to_parquet(outdir / \"events_min.parquet\", index=False)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8) Train twin RAPM-like models: xG For & xG Against\n",
    "    # -----------------------------\n",
    "    targets, appearances = build_match_targets_and_appearances(events_all)\n",
    "    dm = DesignMatrixBuilder(min_app_per_player=min_app_per_player, role_col=\"pos_cluster_id\")\n",
    "    X, mapping, feature_names, y_for, y_against = dm.build(\n",
    "        targets, appearances, clusters[[\"player_id\", \"pos_cluster_id\"]], id_cols_roles=[\"player_id\"]\n",
    "    )\n",
    "    rapm_for, rapm_against = train_two_ridge_models_for_against(X, y_for, y_against)\n",
    "\n",
    "    # === inside export_artifacts(...) ===\n",
    "\n",
    "    # 4bis) Network-effects assets\n",
    "    team_style, team_style_meta = build_team_season_style(events_all)\n",
    "    team_role_mix = build_team_season_role_mix(events_all, clusters)\n",
    "\n",
    "    # Save style + meta + role mix\n",
    "    team_style.to_parquet(outdir / \"team_style.parquet\", index=False)\n",
    "    team_role_mix.to_parquet(outdir / \"team_role_mix.parquet\", index=False)\n",
    "    with open(outdir / \"team_style_meta.pkl\", \"wb\") as f:\n",
    "        pickle.dump(team_style_meta, f)\n",
    "\n",
    "    # Prepare training table\n",
    "    train_df, net_feat_cols, net_tgt_cols = prepare_network_training(\n",
    "        seasonal_table=seasonal_table,\n",
    "        rw_profiles=rw,\n",
    "        clusters=clusters,\n",
    "        team_style=team_style,\n",
    "        team_role_mix=team_role_mix,\n",
    "        keys=[\"player_id\",\"player\"]\n",
    "    )\n",
    "\n",
    "    # Train models per stat\n",
    "    network_models = train_network_effects_models(\n",
    "        train_df=train_df,\n",
    "        feature_cols=net_feat_cols,\n",
    "        target_cols=net_tgt_cols\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    with open(outdir / \"network_models.pkl\", \"wb\") as f:\n",
    "        pickle.dump(network_models, f)\n",
    "    import json\n",
    "    with open(outdir / \"network_feature_cols.json\", \"w\") as f:\n",
    "        json.dump(net_feat_cols, f)\n",
    "    with open(outdir / \"network_target_cols.json\", \"w\") as f:\n",
    "        json.dump(net_tgt_cols, f)\n",
    "\n",
    "\n",
    "    # 9) Save models and metadata\n",
    "    with open(outdir / \"rapm_for.pkl\", \"wb\") as f:\n",
    "        pickle.dump(rapm_for, f)\n",
    "    with open(outdir / \"rapm_against.pkl\", \"wb\") as f:\n",
    "        pickle.dump(rapm_against, f)\n",
    "    with open(outdir / \"feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(feature_names, f)\n",
    "    with open(outdir / \"mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f)\n",
    "\n",
    "    print(f\"✔ Artifacts exported to: {outdir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/2070280149.py:174: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_key_pass\"] = (df.get(\"pass_shot_assist\", False).fillna(False) & df[\"type\"].eq(\"Pass\")).astype(int)\n",
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/2070280149.py:175: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_assist\"]   = (df.get(\"pass_goal_assist\", False).fillna(False) & df[\"type\"].eq(\"Pass\")).astype(int)\n",
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_32259/2070280149.py:188: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  blocks_any = df[block_cols].fillna(False).any(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Assume you already have:\n",
    "# events_all  (full events with columns used above)\n",
    "seasonal_table = build_player_team_season_table(data_all)  # from your earlier step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/zdlsv2353t9ftz425gflmdg80000gn/T/ipykernel_27715/2208615032.py:93: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(topk_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Artifacts exported to: /Users/sdulongsalazar/Desktop/hackathon-america/artifacts\n"
     ]
    }
   ],
   "source": [
    "export_artifacts(\n",
    "    events_all=data_all,\n",
    "    seasonal_table=seasonal_table,\n",
    "    output_dir=\"./artifacts\",     # where the Streamlit app will read from\n",
    "    min_games_pos=1,\n",
    "    min_app_per_player=5,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
